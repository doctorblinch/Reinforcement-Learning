{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33ba5b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0fbb4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the environment.\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92e409a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define and build the DQN.\n",
    "\n",
    "#Some parameters and hyperparameters.\n",
    "input_shape = env.observation_space.shape[0]\n",
    "output_shape = env.action_space.n\n",
    "\n",
    "hidden_size = 64 \n",
    "learning_rate = 0.0001\n",
    "\n",
    "#Build a basic network.\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "\n",
    "class Qnet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_shape = env.observation_space.shape\n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "        self.input_layer = nn.Linear(*self.input_shape, hidden_size)\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, self.output_shape)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.input_layer(x))\n",
    "        x = torch.nn.functional.relu(self.hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a06a215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define and build the memory for replay.\n",
    "\n",
    "#Hyperparameters.\n",
    "memory_size = 10000\n",
    "batch_size = 1500\n",
    "\n",
    "#Definition.\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0ba56db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the agent.\n",
    "\n",
    "#Hyperparameters.\n",
    "gamma = 0.95\n",
    "e_max = 1.0\n",
    "e_min = 0.001\n",
    "e_decay = 0.9995\n",
    "\n",
    "#Definition.\n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.memory = Memory(memory_size)\n",
    "        self.epsilon = e_max\n",
    "        self.policy_network = Qnet()\n",
    "        self.target_network = Qnet()\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        state = torch.tensor(observation).float().detach()\n",
    "        state = state.to(device)\n",
    "        state = state.unsqueeze(0)\n",
    "        q_values = self.policy_network(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "    \n",
    "    def learn(self):\n",
    "        if len(self.memory.buffer) < batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(batch_size)\n",
    "        states = torch.tensor(np.array([each[0] for each in batch]), dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(np.array([each[1] for each in batch]), dtype=torch.long).to(device)\n",
    "        rewards = torch.tensor(np.array([each[2] for each in batch]), dtype=torch.float32).to(device)\n",
    "        states_ = torch.tensor(np.array([each[3] for each in batch]), dtype=torch.float32).to(device)\n",
    "        dones = torch.tensor(np.array([each[4] for each in batch]), dtype=torch.bool).to(device)\n",
    "        batch_indices = np.arange(batch_size, dtype=np.int64)\n",
    "        \n",
    "        q_values = self.policy_network(states)\n",
    "        #next_q_values = self.policy_network(states_) #No target net update.\n",
    "        next_q_values = self.target_network(states_)\n",
    "        \n",
    "        predicted_value_of_now = q_values[batch_indices, actions]\n",
    "        predicted_value_of_future = torch.max(next_q_values, dim=1)[0]\n",
    "        \n",
    "        q_target = rewards + gamma * predicted_value_of_future * dones\n",
    "        \n",
    "        self.epsilon *= e_decay\n",
    "        self.epsilon = max(e_min, self.epsilon)\n",
    "        \n",
    "        loss = self.policy_network.loss(q_target, predicted_value_of_now)\n",
    "        self.policy_network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.policy_network.optimizer.step()\n",
    "            \n",
    "    \n",
    "    def return_epsilon(self):\n",
    "        return self.epsilon\n",
    "    \n",
    "    def update_target_net(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dff37038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Reward 57.0 Epsilon 1.0\n",
      "Episode 10 Reward 32.0 Epsilon 1.0\n",
      "Episode 20 Reward 29.0 Epsilon 1.0\n",
      "Episode 30 Reward 25.0 Epsilon 1.0\n",
      "Episode 40 Reward 22.0 Epsilon 1.0\n",
      "Episode 50 Reward 11.0 Epsilon 1.0\n",
      "Episode 60 Reward 16.0 Epsilon 1.0\n",
      "Episode 70 Reward 9.0 Epsilon 0.9970037475009376\n",
      "Episode 80 Reward 12.0 Epsilon 0.9920299301136144\n",
      "Episode 90 Reward 25.0 Epsilon 0.9870809259323243\n",
      "Episode 100 Reward 11.0 Epsilon 0.9821566111698136\n",
      "Episode 110 Reward 13.0 Epsilon 0.9772568626563747\n",
      "Episode 120 Reward 17.0 Epsilon 0.972381557836764\n",
      "Episode 130 Reward 29.0 Epsilon 0.9675305747671379\n",
      "Episode 140 Reward 15.0 Epsilon 0.9627037921120016\n",
      "Episode 150 Reward 13.0 Epsilon 0.9579010891411742\n",
      "Episode 160 Reward 30.0 Epsilon 0.9531223457267703\n",
      "Episode 170 Reward 28.0 Epsilon 0.9483674423401935\n",
      "Episode 180 Reward 23.0 Epsilon 0.9436362600491477\n",
      "Episode 190 Reward 17.0 Epsilon 0.938928680514662\n",
      "Episode 200 Reward 9.0 Epsilon 0.9342445859881309\n",
      "Episode 210 Reward 13.0 Epsilon 0.9295838593083691\n",
      "Episode 220 Reward 18.0 Epsilon 0.9249463838986807\n",
      "Episode 230 Reward 11.0 Epsilon 0.9203320437639437\n",
      "Episode 240 Reward 22.0 Epsilon 0.9157407234877085\n",
      "Episode 250 Reward 16.0 Epsilon 0.9111723082293108\n",
      "Episode 260 Reward 71.0 Epsilon 0.9066266837209999\n",
      "Episode 270 Reward 10.0 Epsilon 0.9021037362650792\n",
      "Episode 280 Reward 22.0 Epsilon 0.8976033527310644\n",
      "Episode 290 Reward 66.0 Epsilon 0.8931254205528514\n",
      "Episode 300 Reward 20.0 Epsilon 0.8886698277259029\n",
      "Episode 310 Reward 35.0 Epsilon 0.8842364628044451\n",
      "Episode 320 Reward 24.0 Epsilon 0.8798252148986817\n",
      "Episode 330 Reward 36.0 Epsilon 0.8754359736720191\n",
      "Episode 340 Reward 11.0 Epsilon 0.8710686293383069\n",
      "Episode 350 Reward 13.0 Epsilon 0.8667230726590923\n",
      "Episode 360 Reward 47.0 Epsilon 0.8623991949408877\n",
      "Episode 370 Reward 12.0 Epsilon 0.858096888032451\n",
      "Episode 380 Reward 29.0 Epsilon 0.853816044322082\n",
      "Episode 390 Reward 40.0 Epsilon 0.8495565567349297\n",
      "Episode 400 Reward 20.0 Epsilon 0.8453183187303144\n",
      "Episode 410 Reward 19.0 Epsilon 0.8411012242990624\n",
      "Episode 420 Reward 17.0 Epsilon 0.8369051679608553\n",
      "Episode 430 Reward 44.0 Epsilon 0.8327300447615915\n",
      "Episode 440 Reward 49.0 Epsilon 0.8285757502707601\n",
      "Episode 450 Reward 10.0 Epsilon 0.8244421805788297\n",
      "Episode 460 Reward 15.0 Epsilon 0.8203292322946492\n",
      "Episode 470 Reward 10.0 Epsilon 0.8162368025428616\n",
      "Episode 480 Reward 12.0 Epsilon 0.8121647889613309\n",
      "Episode 490 Reward 11.0 Epsilon 0.808113089698582\n",
      "Episode 500 Reward 34.0 Epsilon 0.8040816034112525\n",
      "Episode 510 Reward 23.0 Epsilon 0.8000702292615582\n",
      "Episode 520 Reward 19.0 Epsilon 0.7960788669147714\n",
      "Episode 530 Reward 37.0 Epsilon 0.7921074165367102\n",
      "Episode 540 Reward 27.0 Epsilon 0.7881557787912421\n",
      "Episode 550 Reward 15.0 Epsilon 0.7842238548377993\n",
      "Episode 560 Reward 14.0 Epsilon 0.7803115463289066\n",
      "Episode 570 Reward 15.0 Epsilon 0.776418755407721\n",
      "Episode 580 Reward 14.0 Epsilon 0.7725453847055842\n",
      "Episode 590 Reward 32.0 Epsilon 0.7686913373395875\n",
      "Episode 600 Reward 15.0 Epsilon 0.764856516910148\n",
      "Episode 610 Reward 10.0 Epsilon 0.761040827498598\n",
      "Episode 620 Reward 30.0 Epsilon 0.7572441736647851\n",
      "Episode 630 Reward 15.0 Epsilon 0.7534664604446856\n",
      "Episode 640 Reward 12.0 Epsilon 0.7497075933480289\n",
      "Episode 650 Reward 14.0 Epsilon 0.7459674783559345\n",
      "Episode 660 Reward 19.0 Epsilon 0.7422460219185594\n",
      "Episode 670 Reward 24.0 Epsilon 0.7385431309527595\n",
      "Episode 680 Reward 16.0 Epsilon 0.7348587128397597\n",
      "Episode 690 Reward 21.0 Epsilon 0.7311926754228393\n",
      "Episode 700 Reward 12.0 Epsilon 0.727544927005025\n",
      "Episode 710 Reward 16.0 Epsilon 0.7239153763467987\n",
      "Episode 720 Reward 19.0 Epsilon 0.7203039326638141\n",
      "Episode 730 Reward 19.0 Epsilon 0.7167105056246272\n",
      "Episode 740 Reward 14.0 Epsilon 0.7131350053484364\n",
      "Episode 750 Reward 12.0 Epsilon 0.7095773424028339\n",
      "Episode 760 Reward 16.0 Epsilon 0.7060374278015695\n",
      "Episode 770 Reward 15.0 Epsilon 0.7025151730023247\n",
      "Episode 780 Reward 11.0 Epsilon 0.6990104899044971\n",
      "Episode 790 Reward 10.0 Epsilon 0.6955232908469979\n",
      "Episode 800 Reward 16.0 Epsilon 0.6920534886060592\n",
      "Episode 810 Reward 19.0 Epsilon 0.6886009963930515\n",
      "Episode 820 Reward 12.0 Epsilon 0.6851657278523136\n",
      "Episode 830 Reward 12.0 Epsilon 0.6817475970589922\n",
      "Episode 840 Reward 16.0 Epsilon 0.6783465185168932\n",
      "Episode 850 Reward 28.0 Epsilon 0.6749624071563427\n",
      "Episode 860 Reward 12.0 Epsilon 0.6715951783320595\n",
      "Episode 870 Reward 11.0 Epsilon 0.6682447478210375\n",
      "Episode 880 Reward 13.0 Epsilon 0.6649110318204399\n",
      "Episode 890 Reward 12.0 Epsilon 0.6615939469455022\n",
      "Episode 900 Reward 9.0 Epsilon 0.658293410227447\n",
      "Episode 910 Reward 21.0 Epsilon 0.655009339111409\n",
      "Episode 920 Reward 11.0 Epsilon 0.6517416514543691\n",
      "Episode 930 Reward 13.0 Epsilon 0.6484902655231009\n",
      "Episode 940 Reward 14.0 Epsilon 0.6452550999921252\n",
      "Episode 950 Reward 10.0 Epsilon 0.6420360739416774\n",
      "Episode 960 Reward 12.0 Epsilon 0.6388331068556816\n",
      "Episode 970 Reward 13.0 Epsilon 0.6356461186197387\n",
      "Episode 980 Reward 9.0 Epsilon 0.6324750295191209\n",
      "Episode 990 Reward 10.0 Epsilon 0.6293197602367786\n",
      "Episode 1000 Reward 14.0 Epsilon 0.6261802318513562\n",
      "Episode 1010 Reward 10.0 Epsilon 0.6230563658352183\n",
      "Episode 1020 Reward 11.0 Epsilon 0.6199480840524855\n",
      "Episode 1030 Reward 18.0 Epsilon 0.6168553087570796\n",
      "Episode 1040 Reward 9.0 Epsilon 0.6137779625907799\n",
      "Episode 1050 Reward 11.0 Epsilon 0.6107159685812871\n",
      "Episode 1060 Reward 13.0 Epsilon 0.6076692501402988\n",
      "Episode 1070 Reward 11.0 Epsilon 0.604637731061594\n",
      "Episode 1080 Reward 11.0 Epsilon 0.601621335519126\n",
      "Episode 1090 Reward 13.0 Epsilon 0.5986199880651266\n",
      "Episode 1100 Reward 9.0 Epsilon 0.5956336136282195\n",
      "Episode 1110 Reward 15.0 Epsilon 0.5926621375115408\n",
      "Episode 1120 Reward 17.0 Epsilon 0.5897054853908725\n",
      "Episode 1130 Reward 21.0 Epsilon 0.5867635833127822\n",
      "Episode 1140 Reward 9.0 Epsilon 0.5838363576927736\n",
      "Episode 1150 Reward 18.0 Epsilon 0.5809237353134469\n",
      "Episode 1160 Reward 24.0 Epsilon 0.5780256433226661\n",
      "Episode 1170 Reward 11.0 Epsilon 0.5751420092317381\n",
      "Episode 1180 Reward 20.0 Epsilon 0.5722727609135977\n",
      "Episode 1190 Reward 11.0 Epsilon 0.5694178266010056\n",
      "Episode 1200 Reward 10.0 Epsilon 0.5665771348847519\n",
      "Episode 1210 Reward 26.0 Epsilon 0.5637506147118709\n",
      "Episode 1220 Reward 9.0 Epsilon 0.5609381953838629\n",
      "Episode 1230 Reward 19.0 Epsilon 0.5581398065549269\n",
      "Episode 1240 Reward 10.0 Epsilon 0.5553553782302005\n",
      "Episode 1250 Reward 12.0 Epsilon 0.5525848407640089\n",
      "Episode 1260 Reward 12.0 Epsilon 0.5498281248581236\n",
      "Episode 1270 Reward 13.0 Epsilon 0.5470851615600282\n",
      "Episode 1280 Reward 19.0 Epsilon 0.5443558822611947\n",
      "Episode 1290 Reward 17.0 Epsilon 0.5416402186953665\n",
      "Episode 1300 Reward 13.0 Epsilon 0.5389381029368515\n",
      "Episode 1310 Reward 17.0 Epsilon 0.5362494673988234\n",
      "Episode 1320 Reward 16.0 Epsilon 0.5335742448316299\n",
      "Episode 1330 Reward 23.0 Epsilon 0.5309123683211118\n",
      "Episode 1340 Reward 33.0 Epsilon 0.5282637712869286\n",
      "Episode 1350 Reward 22.0 Epsilon 0.525628387480894\n",
      "Episode 1360 Reward 34.0 Epsilon 0.5230061509853179\n",
      "Episode 1370 Reward 25.0 Epsilon 0.5203969962113582\n",
      "Episode 1380 Reward 75.0 Epsilon 0.5178008578973803\n",
      "Episode 1390 Reward 29.0 Epsilon 0.5152176711073241\n",
      "Episode 1400 Reward 118.0 Epsilon 0.5126473712290808\n",
      "Episode 1410 Reward 52.0 Epsilon 0.510089893972876\n",
      "Episode 1420 Reward 37.0 Epsilon 0.5075451753696616\n",
      "Episode 1430 Reward 98.0 Epsilon 0.5050131517695162\n",
      "Episode 1440 Reward 10.0 Epsilon 0.5024937598400535\n",
      "Episode 1450 Reward 21.0 Epsilon 0.49998693656483667\n",
      "Episode 1460 Reward 88.0 Epsilon 0.4974926192418035\n",
      "Episode 1470 Reward 48.0 Epsilon 0.4950107454816976\n",
      "Episode 1480 Reward 21.0 Epsilon 0.492541253206508\n",
      "Episode 1490 Reward 108.0 Epsilon 0.4900840806479162\n",
      "Episode 1500 Reward 97.0 Epsilon 0.4876391663457515\n",
      "Episode 1510 Reward 66.0 Epsilon 0.48520644914645333\n",
      "Episode 1520 Reward 22.0 Epsilon 0.482785868201542\n",
      "Episode 1530 Reward 26.0 Epsilon 0.48037736296609657\n",
      "Episode 1540 Reward 45.0 Epsilon 0.4779808731972405\n",
      "Episode 1550 Reward 20.0 Epsilon 0.47559633895263465\n",
      "Episode 1560 Reward 48.0 Epsilon 0.47322370058897834\n",
      "Episode 1570 Reward 68.0 Epsilon 0.4708628987605172\n",
      "Episode 1580 Reward 17.0 Epsilon 0.4685138744175588\n",
      "Episode 1590 Reward 36.0 Epsilon 0.4661765688049959\n",
      "Episode 1600 Reward 84.0 Epsilon 0.46385092346083656\n",
      "Episode 1610 Reward 41.0 Epsilon 0.46153688021474204\n",
      "Episode 1620 Reward 49.0 Epsilon 0.4592343811865719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1630 Reward 38.0 Epsilon 0.45694336878493574\n",
      "Episode 1640 Reward 74.0 Epsilon 0.4546637857057534\n",
      "Episode 1650 Reward 29.0 Epsilon 0.4523955749308212\n",
      "Episode 1660 Reward 86.0 Epsilon 0.4501386797263858\n",
      "Episode 1670 Reward 34.0 Epsilon 0.4478930436417254\n",
      "Episode 1680 Reward 49.0 Epsilon 0.4456586105077373\n",
      "Episode 1690 Reward 62.0 Epsilon 0.44343532443553363\n",
      "Episode 1700 Reward 63.0 Epsilon 0.44122312981504286\n",
      "Episode 1710 Reward 61.0 Epsilon 0.4390219713136189\n",
      "Episode 1720 Reward 37.0 Epsilon 0.4368317938746575\n",
      "Episode 1730 Reward 62.0 Epsilon 0.4346525427162186\n",
      "Episode 1740 Reward 44.0 Epsilon 0.43248416332965656\n",
      "Episode 1750 Reward 79.0 Epsilon 0.4303266014782564\n",
      "Episode 1760 Reward 82.0 Epsilon 0.4281798031958776\n",
      "Episode 1770 Reward 44.0 Epsilon 0.4260437147856038\n",
      "Episode 1780 Reward 58.0 Epsilon 0.42391828281840016\n",
      "Episode 1790 Reward 37.0 Epsilon 0.42180345413177645\n",
      "Episode 1800 Reward 51.0 Epsilon 0.4196991758284578\n",
      "Episode 1810 Reward 22.0 Epsilon 0.41760539527506135\n",
      "Episode 1820 Reward 55.0 Epsilon 0.41552206010077997\n",
      "Episode 1830 Reward 37.0 Epsilon 0.41344911819607194\n",
      "Episode 1840 Reward 38.0 Epsilon 0.411386517711358\n",
      "Episode 1850 Reward 68.0 Epsilon 0.40933420705572415\n",
      "Episode 1860 Reward 39.0 Epsilon 0.4072921348956314\n",
      "Episode 1870 Reward 32.0 Epsilon 0.4052602501536315\n",
      "Episode 1880 Reward 113.0 Epsilon 0.4032385020070898\n",
      "Episode 1890 Reward 148.0 Epsilon 0.401226839886914\n",
      "Episode 1900 Reward 123.0 Epsilon 0.39922521347628875\n",
      "Episode 1910 Reward 47.0 Epsilon 0.3972335727094176\n",
      "Episode 1920 Reward 52.0 Epsilon 0.3952518677702707\n",
      "Episode 1930 Reward 117.0 Epsilon 0.3932800490913385\n",
      "Episode 1940 Reward 99.0 Epsilon 0.3913180673523923\n",
      "Episode 1950 Reward 33.0 Epsilon 0.38936587347925017\n",
      "Episode 1960 Reward 59.0 Epsilon 0.38742341864255003\n",
      "Episode 1970 Reward 42.0 Epsilon 0.38549065425652784\n",
      "Episode 1980 Reward 82.0 Epsilon 0.38356753197780247\n",
      "Episode 1990 Reward 111.0 Epsilon 0.3816540037041667\n",
      "Episode 2000 Reward 86.0 Epsilon 0.37975002157338383\n",
      "Episode 2010 Reward 114.0 Epsilon 0.37785553796199073\n",
      "Episode 2020 Reward 41.0 Epsilon 0.3759705054841063\n",
      "Episode 2030 Reward 97.0 Epsilon 0.3740948769902468\n",
      "Episode 2040 Reward 124.0 Epsilon 0.37222860556614584\n",
      "Episode 2050 Reward 91.0 Epsilon 0.3703716445315814\n",
      "Episode 2060 Reward 29.0 Epsilon 0.36852394743920824\n",
      "Episode 2070 Reward 68.0 Epsilon 0.36668546807339586\n",
      "Episode 2080 Reward 71.0 Epsilon 0.3648561604490727\n",
      "Episode 2090 Reward 62.0 Epsilon 0.3630359788105761\n",
      "Episode 2100 Reward 144.0 Epsilon 0.3612248776305073\n",
      "Episode 2110 Reward 63.0 Epsilon 0.3594228116085933\n",
      "Episode 2120 Reward 38.0 Epsilon 0.3576297356705535\n",
      "Episode 2130 Reward 80.0 Epsilon 0.35584560496697226\n",
      "Episode 2140 Reward 128.0 Epsilon 0.3540703748721771\n",
      "Episode 2150 Reward 91.0 Epsilon 0.3523040009831225\n",
      "Episode 2160 Reward 45.0 Epsilon 0.35054643911827926\n",
      "Episode 2170 Reward 28.0 Epsilon 0.3487976453165298\n",
      "Episode 2180 Reward 192.0 Epsilon 0.3470575758360677\n",
      "Episode 2190 Reward 69.0 Epsilon 0.34532618715330443\n",
      "Episode 2200 Reward 83.0 Epsilon 0.3436034359617805\n",
      "Episode 2210 Reward 74.0 Epsilon 0.3418892791710819\n",
      "Episode 2220 Reward 147.0 Epsilon 0.3401836739057629\n",
      "Episode 2230 Reward 29.0 Epsilon 0.33848657750427275\n",
      "Episode 2240 Reward 142.0 Epsilon 0.33679794751788983\n",
      "Episode 2250 Reward 84.0 Epsilon 0.3351177417096587\n",
      "Episode 2260 Reward 142.0 Epsilon 0.3334459180533345\n",
      "Episode 2270 Reward 124.0 Epsilon 0.3317824347323312\n",
      "Episode 2280 Reward 65.0 Epsilon 0.330127250138676\n",
      "Episode 2290 Reward 174.0 Epsilon 0.3284803228719684\n",
      "Episode 2300 Reward 65.0 Epsilon 0.32684161173834486\n",
      "Episode 2310 Reward 117.0 Epsilon 0.32521107574944835\n",
      "Episode 2320 Reward 69.0 Epsilon 0.32358867412140313\n",
      "Episode 2330 Reward 151.0 Epsilon 0.3219743662737947\n",
      "Episode 2340 Reward 49.0 Epsilon 0.32036811182865427\n",
      "Episode 2350 Reward 161.0 Epsilon 0.31876987060944983\n",
      "Episode 2360 Reward 110.0 Epsilon 0.3171796026400804\n",
      "Episode 2370 Reward 322.0 Epsilon 0.31559726814387634\n",
      "Episode 2380 Reward 130.0 Epsilon 0.31402282754260447\n",
      "Episode 2390 Reward 200.0 Epsilon 0.3124562414554782\n",
      "Episode 2400 Reward 112.0 Epsilon 0.31089747069817225\n",
      "Episode 2410 Reward 141.0 Epsilon 0.3093464762818429\n",
      "Episode 2420 Reward 108.0 Epsilon 0.30780321941215266\n",
      "Episode 2430 Reward 45.0 Epsilon 0.30626766148829976\n",
      "Episode 2440 Reward 110.0 Epsilon 0.30473976410205267\n",
      "Episode 2450 Reward 116.0 Epsilon 0.3032194890367895\n",
      "Episode 2460 Reward 42.0 Epsilon 0.30170679826654245\n",
      "Episode 2470 Reward 50.0 Epsilon 0.300201653955046\n",
      "Episode 2480 Reward 147.0 Epsilon 0.29870401845479094\n",
      "Episode 2490 Reward 99.0 Epsilon 0.29721385430608255\n",
      "Episode 2500 Reward 123.0 Epsilon 0.2957311242361041\n",
      "Episode 2510 Reward 123.0 Epsilon 0.2942557911579837\n",
      "Episode 2520 Reward 48.0 Epsilon 0.2927878181698675\n",
      "Episode 2530 Reward 86.0 Epsilon 0.29132716855399593\n",
      "Episode 2540 Reward 137.0 Epsilon 0.28987380577578625\n",
      "Episode 2550 Reward 105.0 Epsilon 0.28842769348291764\n",
      "Episode 2560 Reward 153.0 Epsilon 0.28698879550442286\n",
      "Episode 2570 Reward 147.0 Epsilon 0.28555707584978285\n",
      "Episode 2580 Reward 209.0 Epsilon 0.284132498708027\n",
      "Episode 2590 Reward 148.0 Epsilon 0.282715028446837\n",
      "Episode 2600 Reward 145.0 Epsilon 0.28130462961165603\n",
      "Episode 2610 Reward 134.0 Epsilon 0.2799012669248015\n",
      "Episode 2620 Reward 130.0 Epsilon 0.2785049052845831\n",
      "Episode 2630 Reward 215.0 Epsilon 0.2771155097644244\n",
      "Episode 2640 Reward 123.0 Epsilon 0.2757330456119896\n",
      "Episode 2650 Reward 138.0 Epsilon 0.27435747824831397\n",
      "Episode 2660 Reward 91.0 Epsilon 0.2729887732669394\n",
      "Episode 2670 Reward 107.0 Epsilon 0.2716268964330533\n",
      "Episode 2680 Reward 173.0 Epsilon 0.2702718136826326\n",
      "Episode 2690 Reward 173.0 Epsilon 0.2689234911215915\n",
      "Episode 2700 Reward 136.0 Epsilon 0.2675818950249339\n",
      "Episode 2710 Reward 122.0 Epsilon 0.26624699183590983\n",
      "Episode 2720 Reward 126.0 Epsilon 0.2649187481651761\n",
      "Episode 2730 Reward 120.0 Epsilon 0.2635971307899609\n",
      "Episode 2740 Reward 115.0 Epsilon 0.2622821066532332\n",
      "Episode 2750 Reward 65.0 Epsilon 0.2609736428628759\n",
      "Episode 2760 Reward 122.0 Epsilon 0.2596717066908624\n",
      "Episode 2770 Reward 170.0 Epsilon 0.258376265572439\n",
      "Episode 2780 Reward 188.0 Epsilon 0.2570872871053098\n",
      "Episode 2790 Reward 27.0 Epsilon 0.25580473904882634\n",
      "Episode 2800 Reward 289.0 Epsilon 0.25452858932318095\n",
      "Episode 2810 Reward 62.0 Epsilon 0.2532588060086049\n",
      "Episode 2820 Reward 89.0 Epsilon 0.25199535734456957\n",
      "Episode 2830 Reward 134.0 Epsilon 0.250738211728992\n",
      "Episode 2840 Reward 79.0 Epsilon 0.24948733771744494\n",
      "Episode 2850 Reward 184.0 Epsilon 0.24824270402236964\n",
      "Episode 2860 Reward 113.0 Epsilon 0.2470042795122939\n",
      "Episode 2870 Reward 324.0 Epsilon 0.2457720332110529\n",
      "Episode 2880 Reward 96.0 Epsilon 0.24454593429701468\n",
      "Episode 2890 Reward 134.0 Epsilon 0.24332595210230934\n",
      "Episode 2900 Reward 100.0 Epsilon 0.24211205611206157\n",
      "Episode 2910 Reward 93.0 Epsilon 0.24090421596362763\n",
      "Episode 2920 Reward 87.0 Epsilon 0.23970240144583596\n",
      "Episode 2930 Reward 98.0 Epsilon 0.23850658249823134\n",
      "Episode 2940 Reward 112.0 Epsilon 0.23731672921032318\n",
      "Episode 2950 Reward 148.0 Epsilon 0.23613281182083729\n",
      "Episode 2960 Reward 116.0 Epsilon 0.2349548007169714\n",
      "Episode 2970 Reward 232.0 Epsilon 0.23378266643365464\n",
      "Episode 2980 Reward 163.0 Epsilon 0.2326163796528104\n",
      "Episode 2990 Reward 259.0 Epsilon 0.23145591120262313\n",
      "Episode 3000 Reward 277.0 Epsilon 0.23030123205680855\n",
      "Episode 3010 Reward 176.0 Epsilon 0.22915231333388766\n",
      "Episode 3020 Reward 186.0 Epsilon 0.22800912629646453\n",
      "Episode 3030 Reward 143.0 Epsilon 0.22687164235050708\n",
      "Episode 3040 Reward 158.0 Epsilon 0.2257398330446324\n",
      "Episode 3050 Reward 241.0 Epsilon 0.22461367006939464\n",
      "Episode 3060 Reward 157.0 Epsilon 0.2234931252565773\n",
      "Episode 3070 Reward 181.0 Epsilon 0.22237817057848835\n",
      "Episode 3080 Reward 260.0 Epsilon 0.22126877814725937\n",
      "Episode 3090 Reward 241.0 Epsilon 0.22016492021414813\n",
      "Episode 3100 Reward 227.0 Epsilon 0.21906656916884404\n",
      "Episode 3110 Reward 214.0 Epsilon 0.21797369753877804\n",
      "Episode 3120 Reward 350.0 Epsilon 0.21688627798843532\n",
      "Episode 3130 Reward 143.0 Epsilon 0.21580428331867138\n",
      "Episode 3140 Reward 250.0 Epsilon 0.21472768646603194\n",
      "Episode 3150 Reward 217.0 Epsilon 0.21365646050207598\n",
      "Episode 3160 Reward 103.0 Epsilon 0.212590578632702\n",
      "Episode 3170 Reward 215.0 Epsilon 0.21153001419747813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3180 Reward 166.0 Epsilon 0.21047474066897506\n",
      "Episode 3190 Reward 161.0 Epsilon 0.20942473165210254\n",
      "Episode 3200 Reward 144.0 Epsilon 0.20837996088344932\n",
      "Episode 3210 Reward 210.0 Epsilon 0.20734040223062608\n",
      "Episode 3220 Reward 341.0 Epsilon 0.2063060296916118\n",
      "Episode 3230 Reward 286.0 Epsilon 0.2052768173941036\n",
      "Episode 3240 Reward 252.0 Epsilon 0.20425273959486928\n",
      "Episode 3250 Reward 248.0 Epsilon 0.20323377067910361\n",
      "Episode 3260 Reward 190.0 Epsilon 0.20221988515978762\n",
      "Episode 3270 Reward 239.0 Epsilon 0.20121105767705114\n",
      "Episode 3280 Reward 245.0 Epsilon 0.20020726299753833\n",
      "Episode 3290 Reward 408.0 Epsilon 0.19920847601377664\n",
      "Episode 3300 Reward 224.0 Epsilon 0.19821467174354895\n",
      "Episode 3310 Reward 309.0 Epsilon 0.19722582532926838\n",
      "Episode 3320 Reward 256.0 Epsilon 0.19624191203735675\n",
      "Episode 3330 Reward 169.0 Epsilon 0.1952629072576259\n",
      "Episode 3340 Reward 199.0 Epsilon 0.19428878650266218\n",
      "Episode 3350 Reward 323.0 Epsilon 0.19331952540721387\n",
      "Episode 3360 Reward 500.0 Epsilon 0.19235509972758166\n",
      "Episode 3370 Reward 212.0 Epsilon 0.19139548534101242\n",
      "Episode 3380 Reward 397.0 Epsilon 0.19044065824509587\n",
      "Episode 3390 Reward 287.0 Epsilon 0.18949059455716394\n",
      "Episode 3400 Reward 217.0 Epsilon 0.18854527051369363\n",
      "Episode 3410 Reward 262.0 Epsilon 0.18760466246971264\n",
      "Episode 3420 Reward 298.0 Epsilon 0.18666874689820787\n",
      "Episode 3430 Reward 212.0 Epsilon 0.18573750038953687\n",
      "Episode 3440 Reward 201.0 Epsilon 0.1848108996508425\n",
      "Episode 3450 Reward 473.0 Epsilon 0.18388892150547012\n",
      "Episode 3460 Reward 374.0 Epsilon 0.18297154289238812\n",
      "Episode 3470 Reward 500.0 Epsilon 0.18205874086561075\n",
      "Episode 3480 Reward 264.0 Epsilon 0.18115049259362453\n",
      "Episode 3490 Reward 265.0 Epsilon 0.18024677535881697\n",
      "Episode 3500 Reward 372.0 Epsilon 0.1793475665569085\n",
      "Episode 3510 Reward 298.0 Epsilon 0.17845284369638684\n",
      "Episode 3520 Reward 292.0 Epsilon 0.17756258439794476\n",
      "Episode 3530 Reward 260.0 Epsilon 0.17667676639391994\n",
      "Episode 3540 Reward 163.0 Epsilon 0.17579536752773836\n",
      "Episode 3550 Reward 197.0 Epsilon 0.1749183657533599\n",
      "Episode 3560 Reward 140.0 Epsilon 0.17404573913472685\n",
      "Episode 3570 Reward 292.0 Epsilon 0.17317746584521543\n",
      "Episode 3580 Reward 199.0 Epsilon 0.17231352416708978\n",
      "Episode 3590 Reward 164.0 Epsilon 0.17145389249095871\n",
      "Episode 3600 Reward 164.0 Epsilon 0.17059854931523513\n",
      "Episode 3610 Reward 152.0 Epsilon 0.16974747324559833\n",
      "Episode 3620 Reward 332.0 Epsilon 0.16890064299445892\n",
      "Episode 3630 Reward 500.0 Epsilon 0.16805803738042624\n",
      "Episode 3640 Reward 427.0 Epsilon 0.1672196353277786\n",
      "Episode 3650 Reward 310.0 Epsilon 0.16638541586593614\n",
      "Episode 3660 Reward 260.0 Epsilon 0.1655553581289363\n",
      "Episode 3670 Reward 309.0 Epsilon 0.1647294413549119\n",
      "Episode 3680 Reward 347.0 Epsilon 0.16390764488557183\n",
      "Episode 3690 Reward 332.0 Epsilon 0.1630899481656843\n",
      "Episode 3700 Reward 356.0 Epsilon 0.16227633074256287\n",
      "Episode 3710 Reward 343.0 Epsilon 0.16146677226555464\n",
      "Episode 3720 Reward 222.0 Epsilon 0.16066125248553134\n",
      "Episode 3730 Reward 239.0 Epsilon 0.15985975125438284\n",
      "Episode 3740 Reward 232.0 Epsilon 0.15906224852451326\n",
      "Episode 3750 Reward 168.0 Epsilon 0.15826872434833938\n",
      "Episode 3760 Reward 207.0 Epsilon 0.15747915887779185\n",
      "Episode 3770 Reward 162.0 Epsilon 0.15669353236381867\n",
      "Episode 3780 Reward 135.0 Epsilon 0.1559118251558911\n",
      "Episode 3790 Reward 175.0 Epsilon 0.15513401770151247\n",
      "Episode 3800 Reward 164.0 Epsilon 0.15436009054572875\n",
      "Episode 3810 Reward 205.0 Epsilon 0.1535900243306422\n",
      "Episode 3820 Reward 229.0 Epsilon 0.15282379979492705\n",
      "Episode 3830 Reward 139.0 Epsilon 0.15206139777334782\n",
      "Episode 3840 Reward 228.0 Epsilon 0.15130279919627976\n",
      "Episode 3850 Reward 214.0 Epsilon 0.15054798508923203\n",
      "Episode 3860 Reward 164.0 Epsilon 0.1497969365723732\n",
      "Episode 3870 Reward 287.0 Epsilon 0.14904963486005865\n",
      "Episode 3880 Reward 282.0 Epsilon 0.1483060612603612\n",
      "Episode 3890 Reward 168.0 Epsilon 0.14756619717460304\n",
      "Episode 3900 Reward 204.0 Epsilon 0.14683002409689105\n",
      "Episode 3910 Reward 225.0 Epsilon 0.1460975236136534\n",
      "Episode 3920 Reward 218.0 Epsilon 0.14536867740317935\n",
      "Episode 3930 Reward 193.0 Epsilon 0.14464346723516094\n",
      "Episode 3940 Reward 205.0 Epsilon 0.1439218749702369\n",
      "Episode 3950 Reward 161.0 Epsilon 0.14320388255953895\n",
      "Episode 3960 Reward 142.0 Epsilon 0.1424894720442403\n",
      "Episode 3970 Reward 162.0 Epsilon 0.14177862555510667\n",
      "Episode 3980 Reward 190.0 Epsilon 0.14107132531204905\n",
      "Episode 3990 Reward 195.0 Epsilon 0.1403675536236792\n",
      "Episode 4000 Reward 146.0 Epsilon 0.13966729288686708\n",
      "Episode 4010 Reward 203.0 Epsilon 0.13897052558630044\n",
      "Episode 4020 Reward 238.0 Epsilon 0.13827723429404684\n",
      "Episode 4030 Reward 283.0 Epsilon 0.13758740166911776\n",
      "Episode 4040 Reward 298.0 Epsilon 0.13690101045703473\n",
      "Episode 4050 Reward 185.0 Epsilon 0.13621804348939787\n",
      "Episode 4060 Reward 500.0 Epsilon 0.13553848368345633\n",
      "Episode 4070 Reward 409.0 Epsilon 0.1348623140416812\n",
      "Episode 4080 Reward 214.0 Epsilon 0.13418951765134016\n",
      "Episode 4090 Reward 136.0 Epsilon 0.13352007768407453\n",
      "Episode 4100 Reward 373.0 Epsilon 0.13285397739547844\n",
      "Episode 4110 Reward 213.0 Epsilon 0.13219120012467986\n",
      "Episode 4120 Reward 245.0 Epsilon 0.13153172929392393\n",
      "Episode 4130 Reward 241.0 Epsilon 0.13087554840815832\n",
      "Episode 4140 Reward 214.0 Epsilon 0.1302226410546207\n",
      "Episode 4150 Reward 183.0 Epsilon 0.1295729909024281\n",
      "Episode 4160 Reward 157.0 Epsilon 0.12892658170216845\n",
      "Episode 4170 Reward 168.0 Epsilon 0.12828339728549426\n",
      "Episode 4180 Reward 179.0 Epsilon 0.12764342156471808\n",
      "Episode 4190 Reward 157.0 Epsilon 0.12700663853241015\n",
      "Episode 4200 Reward 166.0 Epsilon 0.12637303226099805\n",
      "Episode 4210 Reward 155.0 Epsilon 0.12574258690236825\n",
      "Episode 4220 Reward 173.0 Epsilon 0.12511528668746974\n",
      "Episode 4230 Reward 173.0 Epsilon 0.12449111592591955\n",
      "Episode 4240 Reward 192.0 Epsilon 0.12387005900561042\n",
      "Episode 4250 Reward 182.0 Epsilon 0.1232521003923202\n",
      "Episode 4260 Reward 158.0 Epsilon 0.12263722462932329\n",
      "Episode 4270 Reward 143.0 Epsilon 0.1220254163370041\n",
      "Episode 4280 Reward 228.0 Epsilon 0.12141666021247231\n",
      "Episode 4290 Reward 153.0 Epsilon 0.12081094102918016\n",
      "Episode 4300 Reward 241.0 Epsilon 0.12020824363654148\n",
      "Episode 4310 Reward 170.0 Epsilon 0.11960855295955289\n",
      "Episode 4320 Reward 158.0 Epsilon 0.11901185399841661\n",
      "Episode 4330 Reward 164.0 Epsilon 0.11841813182816537\n",
      "Episode 4340 Reward 159.0 Epsilon 0.117827371598289\n",
      "Episode 4350 Reward 132.0 Epsilon 0.11723955853236308\n",
      "Episode 4360 Reward 150.0 Epsilon 0.11665467792767928\n",
      "Episode 4370 Reward 130.0 Epsilon 0.11607271515487763\n",
      "Episode 4380 Reward 142.0 Epsilon 0.1154936556575806\n",
      "Episode 4390 Reward 120.0 Epsilon 0.11491748495202903\n",
      "Episode 4400 Reward 143.0 Epsilon 0.11434418862671976\n",
      "Episode 4410 Reward 151.0 Epsilon 0.11377375234204529\n",
      "Episode 4420 Reward 228.0 Epsilon 0.11320616182993508\n",
      "Episode 4430 Reward 302.0 Epsilon 0.11264140289349857\n",
      "Episode 4440 Reward 191.0 Epsilon 0.11207946140667016\n",
      "Episode 4450 Reward 305.0 Epsilon 0.11152032331385595\n",
      "Episode 4460 Reward 276.0 Epsilon 0.11096397462958196\n",
      "Episode 4470 Reward 303.0 Epsilon 0.11041040143814458\n",
      "Episode 4480 Reward 369.0 Epsilon 0.10985958989326235\n",
      "Episode 4490 Reward 247.0 Epsilon 0.10931152621772958\n",
      "Episode 4500 Reward 376.0 Epsilon 0.1087661967030719\n",
      "Episode 4510 Reward 216.0 Epsilon 0.10822358770920325\n",
      "Episode 4520 Reward 194.0 Epsilon 0.1076836856640848\n",
      "Episode 4530 Reward 247.0 Epsilon 0.10714647706338536\n",
      "Episode 4540 Reward 181.0 Epsilon 0.10661194847014373\n",
      "Episode 4550 Reward 166.0 Epsilon 0.10608008651443257\n",
      "Episode 4560 Reward 134.0 Epsilon 0.10555087789302389\n",
      "Episode 4570 Reward 136.0 Epsilon 0.10502430936905649\n",
      "Episode 4580 Reward 128.0 Epsilon 0.10450036777170466\n",
      "Episode 4590 Reward 125.0 Epsilon 0.10397903999584887\n",
      "Episode 4600 Reward 149.0 Epsilon 0.10346031300174793\n",
      "Episode 4610 Reward 166.0 Epsilon 0.10294417381471291\n",
      "Episode 4620 Reward 250.0 Epsilon 0.10243060952478254\n",
      "Episode 4630 Reward 181.0 Epsilon 0.10191960728640029\n",
      "Episode 4640 Reward 207.0 Epsilon 0.10141115431809308\n",
      "Episode 4650 Reward 270.0 Epsilon 0.10090523790215165\n",
      "Episode 4660 Reward 184.0 Epsilon 0.10040184538431238\n",
      "Episode 4670 Reward 212.0 Epsilon 0.09990096417344076\n",
      "Episode 4680 Reward 279.0 Epsilon 0.09940258174121651\n",
      "Episode 4690 Reward 227.0 Epsilon 0.0989066856218202\n",
      "Episode 4700 Reward 255.0 Epsilon 0.09841326341162147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4710 Reward 165.0 Epsilon 0.09792230276886875\n",
      "Episode 4720 Reward 175.0 Epsilon 0.09743379141338052\n",
      "Episode 4730 Reward 148.0 Epsilon 0.09694771712623826\n",
      "Episode 4740 Reward 156.0 Epsilon 0.09646406774948073\n",
      "Episode 4750 Reward 137.0 Epsilon 0.0959828311857999\n",
      "Episode 4760 Reward 126.0 Epsilon 0.09550399539823835\n",
      "Episode 4770 Reward 119.0 Epsilon 0.09502754840988827\n",
      "Episode 4780 Reward 124.0 Epsilon 0.09455347830359179\n",
      "Episode 4790 Reward 134.0 Epsilon 0.09408177322164288\n",
      "Episode 4800 Reward 120.0 Epsilon 0.09361242136549094\n",
      "Episode 4810 Reward 151.0 Epsilon 0.09314541099544553\n",
      "Episode 4820 Reward 189.0 Epsilon 0.09268073043038266\n",
      "Episode 4830 Reward 180.0 Epsilon 0.09221836804745288\n",
      "Episode 4840 Reward 151.0 Epsilon 0.09175831228179032\n",
      "Episode 4850 Reward 204.0 Epsilon 0.09130055162622351\n",
      "Episode 4860 Reward 156.0 Epsilon 0.09084507463098758\n",
      "Episode 4870 Reward 134.0 Epsilon 0.09039186990343778\n",
      "Episode 4880 Reward 140.0 Epsilon 0.0899409261077647\n",
      "Episode 4890 Reward 146.0 Epsilon 0.08949223196471051\n",
      "Episode 4900 Reward 155.0 Epsilon 0.08904577625128701\n",
      "Episode 4910 Reward 146.0 Epsilon 0.08860154780049484\n",
      "Episode 4920 Reward 131.0 Epsilon 0.08815953550104416\n",
      "Episode 4930 Reward 160.0 Epsilon 0.08771972829707674\n",
      "Episode 4940 Reward 151.0 Epsilon 0.08728211518788946\n",
      "Episode 4950 Reward 127.0 Epsilon 0.0868466852276591\n",
      "Episode 4960 Reward 145.0 Epsilon 0.08641342752516859\n",
      "Episode 4970 Reward 155.0 Epsilon 0.08598233124353452\n",
      "Episode 4980 Reward 133.0 Epsilon 0.08555338559993626\n",
      "Episode 4990 Reward 146.0 Epsilon 0.08512657986534605\n",
      "Episode 5000 Reward 138.0 Epsilon 0.08470190336426077\n",
      "Episode 5010 Reward 136.0 Epsilon 0.08427934547443484\n",
      "Episode 5020 Reward 173.0 Epsilon 0.08385889562661462\n",
      "Episode 5030 Reward 196.0 Epsilon 0.08344054330427397\n",
      "Episode 5040 Reward 188.0 Epsilon 0.08302427804335118\n",
      "Episode 5050 Reward 150.0 Epsilon 0.08261008943198732\n",
      "Episode 5060 Reward 146.0 Epsilon 0.08219796711026581\n",
      "Episode 5070 Reward 148.0 Epsilon 0.0817879007699532\n",
      "Episode 5080 Reward 176.0 Epsilon 0.08137988015424144\n",
      "Episode 5090 Reward 139.0 Epsilon 0.08097389505749128\n",
      "Episode 5100 Reward 140.0 Epsilon 0.08056993532497697\n",
      "Episode 5110 Reward 154.0 Epsilon 0.08016799085263235\n",
      "Episode 5120 Reward 140.0 Epsilon 0.07976805158679803\n",
      "Episode 5130 Reward 136.0 Epsilon 0.07937010752396999\n",
      "Episode 5140 Reward 153.0 Epsilon 0.07897414871054934\n",
      "Episode 5150 Reward 151.0 Epsilon 0.0785801652425933\n",
      "Episode 5160 Reward 139.0 Epsilon 0.0781881472655676\n",
      "Episode 5170 Reward 132.0 Epsilon 0.07779808497409992\n",
      "Episode 5180 Reward 183.0 Epsilon 0.07740996861173462\n",
      "Episode 5190 Reward 163.0 Epsilon 0.07702378847068873\n",
      "Episode 5200 Reward 160.0 Epsilon 0.07663953489160913\n",
      "Episode 5210 Reward 137.0 Epsilon 0.07625719826333091\n",
      "Episode 5220 Reward 129.0 Epsilon 0.07587676902263708\n",
      "Episode 5230 Reward 137.0 Epsilon 0.07549823765401918\n",
      "Episode 5240 Reward 144.0 Epsilon 0.07512159468943949\n",
      "Episode 5250 Reward 129.0 Epsilon 0.07474683070809404\n",
      "Episode 5260 Reward 124.0 Epsilon 0.07437393633617707\n",
      "Episode 5270 Reward 115.0 Epsilon 0.07400290224664655\n",
      "Episode 5280 Reward 132.0 Epsilon 0.0736337191589908\n",
      "Episode 5290 Reward 176.0 Epsilon 0.0732663778389965\n",
      "Episode 5300 Reward 143.0 Epsilon 0.07290086909851762\n",
      "Episode 5310 Reward 124.0 Epsilon 0.07253718379524565\n",
      "Episode 5320 Reward 131.0 Epsilon 0.0721753128324809\n",
      "Episode 5330 Reward 140.0 Epsilon 0.07181524715890494\n",
      "Episode 5340 Reward 158.0 Epsilon 0.07145697776835429\n",
      "Episode 5350 Reward 119.0 Epsilon 0.0711004956995951\n",
      "Episode 5360 Reward 138.0 Epsilon 0.070745792036099\n",
      "Episode 5370 Reward 127.0 Epsilon 0.07039285790582007\n",
      "Episode 5380 Reward 127.0 Epsilon 0.07004168448097293\n",
      "Episode 5390 Reward 137.0 Epsilon 0.069692262977812\n",
      "Episode 5400 Reward 131.0 Epsilon 0.0693445846564117\n",
      "Episode 5410 Reward 143.0 Epsilon 0.0689986408204479\n",
      "Episode 5420 Reward 183.0 Epsilon 0.06865442281698036\n",
      "Episode 5430 Reward 176.0 Epsilon 0.0683119220362364\n",
      "Episode 5440 Reward 166.0 Epsilon 0.06797112991139537\n",
      "Episode 5450 Reward 183.0 Epsilon 0.06763203791837455\n",
      "Episode 5460 Reward 164.0 Epsilon 0.06729463757561584\n",
      "Episode 5470 Reward 131.0 Epsilon 0.06695892044387368\n",
      "Episode 5480 Reward 147.0 Epsilon 0.06662487812600386\n",
      "Episode 5490 Reward 146.0 Epsilon 0.06629250226675355\n",
      "Episode 5500 Reward 150.0 Epsilon 0.06596178455255233\n",
      "Episode 5510 Reward 137.0 Epsilon 0.06563271671130429\n",
      "Episode 5520 Reward 124.0 Epsilon 0.06530529051218098\n",
      "Episode 5530 Reward 133.0 Epsilon 0.06497949776541559\n",
      "Episode 5540 Reward 147.0 Epsilon 0.06465533032209828\n",
      "Episode 5550 Reward 136.0 Epsilon 0.06433278007397207\n",
      "Episode 5560 Reward 167.0 Epsilon 0.06401183895323023\n",
      "Episode 5570 Reward 150.0 Epsilon 0.0636924989323144\n",
      "Episode 5580 Reward 139.0 Epsilon 0.06337475202371384\n",
      "Episode 5590 Reward 132.0 Epsilon 0.0630585902797656\n",
      "Episode 5600 Reward 121.0 Epsilon 0.06274400579245575\n",
      "Episode 5610 Reward 148.0 Epsilon 0.06243099069322161\n",
      "Episode 5620 Reward 145.0 Epsilon 0.062119537152754895\n",
      "Episode 5630 Reward 150.0 Epsilon 0.06180963738080588\n",
      "Episode 5640 Reward 149.0 Epsilon 0.061501283625988626\n",
      "Episode 5650 Reward 145.0 Epsilon 0.06119446817558698\n",
      "Episode 5660 Reward 158.0 Epsilon 0.06088918335536172\n",
      "Episode 5670 Reward 140.0 Epsilon 0.06058542152935864\n",
      "Episode 5680 Reward 133.0 Epsilon 0.06028317509971747\n",
      "Episode 5690 Reward 135.0 Epsilon 0.0599824365064819\n",
      "Episode 5700 Reward 146.0 Epsilon 0.059683198227410465\n",
      "Episode 5710 Reward 134.0 Epsilon 0.059385452777788415\n",
      "Episode 5720 Reward 129.0 Epsilon 0.059089192710240435\n",
      "Episode 5730 Reward 138.0 Epsilon 0.05879441061454444\n",
      "Episode 5740 Reward 132.0 Epsilon 0.05850109911744622\n",
      "Episode 5750 Reward 126.0 Epsilon 0.058209250882474965\n",
      "Episode 5760 Reward 125.0 Epsilon 0.05791885860975983\n",
      "Episode 5770 Reward 121.0 Epsilon 0.05762991503584728\n",
      "Episode 5780 Reward 126.0 Epsilon 0.057342412933519445\n",
      "Episode 5790 Reward 126.0 Epsilon 0.057056345111613345\n",
      "Episode 5800 Reward 133.0 Epsilon 0.056771704414841064\n",
      "Episode 5810 Reward 133.0 Epsilon 0.05648848372361067\n",
      "Episode 5820 Reward 240.0 Epsilon 0.05620667595384825\n",
      "Episode 5830 Reward 131.0 Epsilon 0.05592627405682066\n",
      "Episode 5840 Reward 124.0 Epsilon 0.0556472710189592\n",
      "Episode 5850 Reward 123.0 Epsilon 0.05536965986168427\n",
      "Episode 5860 Reward 119.0 Epsilon 0.05509343364123071\n",
      "Episode 5870 Reward 123.0 Epsilon 0.05481858544847421\n",
      "Episode 5880 Reward 137.0 Epsilon 0.05454510840875845\n",
      "Episode 5890 Reward 123.0 Epsilon 0.05427299568172313\n",
      "Episode 5900 Reward 122.0 Epsilon 0.05400224046113293\n",
      "Episode 5910 Reward 171.0 Epsilon 0.05373283597470724\n",
      "Episode 5920 Reward 147.0 Epsilon 0.053464775483950726\n",
      "Episode 5930 Reward 139.0 Epsilon 0.05319805228398488\n",
      "Episode 5940 Reward 144.0 Epsilon 0.052932659703380215\n",
      "Episode 5950 Reward 151.0 Epsilon 0.05266859110398944\n",
      "Episode 5960 Reward 151.0 Epsilon 0.052405839880781444\n",
      "Episode 5970 Reward 126.0 Epsilon 0.05214439946167604\n",
      "Episode 5980 Reward 146.0 Epsilon 0.051884263307379615\n",
      "Episode 5990 Reward 126.0 Epsilon 0.051625424911221574\n",
      "Episode 6000 Reward 216.0 Epsilon 0.05136787779899155\n",
      "Episode 6010 Reward 119.0 Epsilon 0.05111161552877749\n",
      "Episode 6020 Reward 128.0 Epsilon 0.05085663169080453\n",
      "Episode 6030 Reward 123.0 Epsilon 0.050602919907274675\n",
      "Episode 6040 Reward 114.0 Epsilon 0.050350473832207254\n",
      "Episode 6050 Reward 105.0 Epsilon 0.05009928715128019\n",
      "Episode 6060 Reward 102.0 Epsilon 0.04984935358167211\n",
      "Episode 6070 Reward 140.0 Epsilon 0.04960066687190514\n",
      "Episode 6080 Reward 111.0 Epsilon 0.049353220801688566\n",
      "Episode 6090 Reward 108.0 Epsilon 0.04910700918176323\n",
      "Episode 6100 Reward 116.0 Epsilon 0.04886202585374675\n",
      "Episode 6110 Reward 99.0 Epsilon 0.04861826468997943\n",
      "Episode 6120 Reward 32.0 Epsilon 0.04837571959337107\n",
      "Episode 6130 Reward 106.0 Epsilon 0.04813438449724842\n",
      "Episode 6140 Reward 91.0 Epsilon 0.047894253365203414\n",
      "Episode 6150 Reward 99.0 Epsilon 0.047655320190942214\n",
      "Episode 6160 Reward 35.0 Epsilon 0.04741757899813498\n",
      "Episode 6170 Reward 28.0 Epsilon 0.04718102384026636\n",
      "Episode 6180 Reward 34.0 Epsilon 0.04694564880048679\n",
      "Episode 6190 Reward 24.0 Epsilon 0.04671144799146444\n",
      "Episode 6200 Reward 24.0 Epsilon 0.04647841555523806\n",
      "Episode 6210 Reward 35.0 Epsilon 0.04624654566307032\n",
      "Episode 6220 Reward 31.0 Epsilon 0.046015832515302134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6230 Reward 97.0 Epsilon 0.04578627034120755\n",
      "Episode 6240 Reward 21.0 Epsilon 0.04555785339884941\n",
      "Episode 6250 Reward 43.0 Epsilon 0.045330575974935707\n",
      "Episode 6260 Reward 23.0 Epsilon 0.045104432384676735\n",
      "Episode 6270 Reward 25.0 Epsilon 0.044879416971642855\n",
      "Episode 6280 Reward 29.0 Epsilon 0.04465552410762303\n",
      "Episode 6290 Reward 39.0 Epsilon 0.044432748192484044\n",
      "Episode 6300 Reward 31.0 Epsilon 0.04421108365403042\n",
      "Episode 6310 Reward 39.0 Epsilon 0.04399052494786505\n",
      "Episode 6320 Reward 23.0 Epsilon 0.043771066557250576\n",
      "Episode 6330 Reward 23.0 Epsilon 0.043552702992971266\n",
      "Episode 6340 Reward 20.0 Epsilon 0.04333542879319585\n",
      "Episode 6350 Reward 23.0 Epsilon 0.04311923852334082\n",
      "Episode 6360 Reward 28.0 Epsilon 0.042904126775934553\n",
      "Episode 6370 Reward 27.0 Epsilon 0.04269008817048201\n",
      "Episode 6380 Reward 25.0 Epsilon 0.04247711735333018\n",
      "Episode 6390 Reward 35.0 Epsilon 0.04226520899753419\n",
      "Episode 6400 Reward 30.0 Epsilon 0.04205435780272403\n",
      "Episode 6410 Reward 26.0 Epsilon 0.04184455849497197\n",
      "Episode 6420 Reward 25.0 Epsilon 0.0416358058266607\n",
      "Episode 6430 Reward 29.0 Epsilon 0.041428094576352034\n",
      "Episode 6440 Reward 26.0 Epsilon 0.041221419548656275\n",
      "Episode 6450 Reward 22.0 Epsilon 0.041015775574102346\n",
      "Episode 6460 Reward 26.0 Epsilon 0.04081115750900845\n",
      "Episode 6470 Reward 32.0 Epsilon 0.04060756023535337\n",
      "Episode 6480 Reward 34.0 Epsilon 0.04040497866064852\n",
      "Episode 6490 Reward 20.0 Epsilon 0.040203407717810544\n",
      "Episode 6500 Reward 42.0 Epsilon 0.04000284236503457\n",
      "Episode 6510 Reward 103.0 Epsilon 0.03980327758566811\n",
      "Episode 6520 Reward 40.0 Epsilon 0.03960470838808557\n",
      "Episode 6530 Reward 38.0 Epsilon 0.0394071298055634\n",
      "Episode 6540 Reward 98.0 Epsilon 0.039210536896155876\n",
      "Episode 6550 Reward 112.0 Epsilon 0.03901492474257148\n",
      "Episode 6560 Reward 104.0 Epsilon 0.0388202884520499\n",
      "Episode 6570 Reward 370.0 Epsilon 0.03862662315623964\n",
      "Episode 6580 Reward 500.0 Epsilon 0.03843392401107629\n",
      "Episode 6590 Reward 21.0 Epsilon 0.03824218619666133\n",
      "Episode 6600 Reward 16.0 Epsilon 0.03805140491714159\n",
      "Episode 6610 Reward 13.0 Epsilon 0.03786157540058927\n",
      "Episode 6620 Reward 21.0 Epsilon 0.03767269289888261\n",
      "Episode 6630 Reward 114.0 Epsilon 0.037484752687587095\n",
      "Episode 6640 Reward 156.0 Epsilon 0.03729775006583731\n",
      "Episode 6650 Reward 500.0 Epsilon 0.037111680356219374\n",
      "Episode 6660 Reward 97.0 Epsilon 0.036926538904653895\n",
      "Episode 6670 Reward 105.0 Epsilon 0.03674232108027961\n",
      "Episode 6680 Reward 127.0 Epsilon 0.03655902227533753\n",
      "Episode 6690 Reward 185.0 Epsilon 0.03637663790505567\n",
      "Episode 6700 Reward 500.0 Epsilon 0.03619516340753443\n",
      "Episode 6710 Reward 445.0 Epsilon 0.03601459424363244\n",
      "Episode 6720 Reward 295.0 Epsilon 0.03583492589685304\n",
      "Episode 6730 Reward 251.0 Epsilon 0.035656153873231296\n",
      "Episode 6740 Reward 288.0 Epsilon 0.03547827370122163\n",
      "Episode 6750 Reward 268.0 Epsilon 0.03530128093158595\n",
      "Episode 6760 Reward 300.0 Epsilon 0.03512517113728236\n",
      "Episode 6770 Reward 220.0 Epsilon 0.03494993991335444\n",
      "Episode 6780 Reward 232.0 Epsilon 0.03477558287682106\n",
      "Episode 6790 Reward 233.0 Epsilon 0.03460209566656675\n",
      "Episode 6800 Reward 211.0 Epsilon 0.03442947394323263\n",
      "Episode 6810 Reward 200.0 Epsilon 0.03425771338910785\n",
      "Episode 6820 Reward 228.0 Epsilon 0.03408680970802162\n",
      "Episode 6830 Reward 210.0 Epsilon 0.03391675862523572\n",
      "Episode 6840 Reward 193.0 Epsilon 0.03374755588733761\n",
      "Episode 6850 Reward 201.0 Epsilon 0.033579197262134\n",
      "Episode 6860 Reward 188.0 Epsilon 0.03341167853854504\n",
      "Episode 6870 Reward 203.0 Epsilon 0.03324499552649897\n",
      "Episode 6880 Reward 202.0 Epsilon 0.033079144056827305\n",
      "Episode 6890 Reward 205.0 Epsilon 0.03291411998116057\n",
      "Episode 6900 Reward 203.0 Epsilon 0.03274991917182452\n",
      "Episode 6910 Reward 189.0 Epsilon 0.03258653752173689\n",
      "Episode 6920 Reward 188.0 Epsilon 0.03242397094430473\n",
      "Episode 6930 Reward 188.0 Epsilon 0.03226221537332209\n",
      "Episode 6940 Reward 173.0 Epsilon 0.032101266762868404\n",
      "Episode 6950 Reward 189.0 Epsilon 0.031941121087207244\n",
      "Episode 6960 Reward 189.0 Epsilon 0.03178177434068561\n",
      "Episode 6970 Reward 175.0 Epsilon 0.03162322253763381\n",
      "Episode 6980 Reward 181.0 Epsilon 0.03146546171226567\n",
      "Episode 6990 Reward 177.0 Epsilon 0.03130848791857944\n",
      "Episode 7000 Reward 165.0 Epsilon 0.031152297230259002\n",
      "Episode 7010 Reward 157.0 Epsilon 0.03099688574057573\n",
      "Episode 7020 Reward 155.0 Epsilon 0.030842249562290754\n",
      "Episode 7030 Reward 167.0 Epsilon 0.03068838482755771\n",
      "Episode 7040 Reward 149.0 Epsilon 0.03053528768782602\n",
      "Episode 7050 Reward 155.0 Epsilon 0.03038295431374461\n",
      "Episode 7060 Reward 163.0 Epsilon 0.030231380895066155\n",
      "Episode 7070 Reward 153.0 Epsilon 0.030080563640551748\n",
      "Episode 7080 Reward 140.0 Epsilon 0.029930498777876083\n",
      "Episode 7090 Reward 150.0 Epsilon 0.029781182553533087\n",
      "Episode 7100 Reward 137.0 Epsilon 0.02963261123274207\n",
      "Episode 7110 Reward 136.0 Epsilon 0.02948478109935427\n",
      "Episode 7120 Reward 138.0 Epsilon 0.029337688455759924\n",
      "Episode 7130 Reward 148.0 Epsilon 0.029191329622795772\n",
      "Episode 7140 Reward 146.0 Epsilon 0.029045700939653023\n",
      "Episode 7150 Reward 151.0 Epsilon 0.02890079876378582\n",
      "Episode 7160 Reward 144.0 Epsilon 0.028756619470820116\n",
      "Episode 7170 Reward 151.0 Epsilon 0.028613159454462987\n",
      "Episode 7180 Reward 154.0 Epsilon 0.028470415126412487\n",
      "Episode 7190 Reward 147.0 Epsilon 0.02832838291626784\n",
      "Episode 7200 Reward 138.0 Epsilon 0.02818705927144017\n",
      "Episode 7210 Reward 140.0 Epsilon 0.02804644065706363\n",
      "Episode 7220 Reward 127.0 Epsilon 0.027906523555906997\n",
      "Episode 7230 Reward 117.0 Epsilon 0.027767304468285684\n",
      "Episode 7240 Reward 126.0 Epsilon 0.027628779911974204\n",
      "Episode 7250 Reward 127.0 Epsilon 0.027490946422119068\n",
      "Episode 7260 Reward 121.0 Epsilon 0.027353800551152146\n",
      "Episode 7270 Reward 118.0 Epsilon 0.027217338868704387\n",
      "Episode 7280 Reward 121.0 Epsilon 0.02708155796152008\n",
      "Episode 7290 Reward 115.0 Epsilon 0.026946454433371424\n",
      "Episode 7300 Reward 36.0 Epsilon 0.0268120249049736\n",
      "Episode 7310 Reward 124.0 Epsilon 0.02667826601390025\n",
      "Episode 7320 Reward 127.0 Epsilon 0.026545174414499377\n",
      "Episode 7330 Reward 118.0 Epsilon 0.026412746777809634\n",
      "Episode 7340 Reward 28.0 Epsilon 0.0262809797914771\n",
      "Episode 7350 Reward 115.0 Epsilon 0.026149870159672413\n",
      "Episode 7360 Reward 21.0 Epsilon 0.02601941460300831\n",
      "Episode 7370 Reward 21.0 Epsilon 0.025889609858457646\n",
      "Episode 7380 Reward 20.0 Epsilon 0.025760452679271725\n",
      "Episode 7390 Reward 26.0 Epsilon 0.025631939834899135\n",
      "Episode 7400 Reward 24.0 Epsilon 0.02550406811090492\n",
      "Episode 7410 Reward 15.0 Epsilon 0.025376834308890168\n",
      "Episode 7420 Reward 19.0 Epsilon 0.02525023524641206\n",
      "Episode 7430 Reward 20.0 Epsilon 0.025124267756904215\n",
      "Episode 7440 Reward 17.0 Epsilon 0.024998928689597518\n",
      "Episode 7450 Reward 17.0 Epsilon 0.02487421490944129\n",
      "Episode 7460 Reward 13.0 Epsilon 0.024750123297024883\n",
      "Episode 7470 Reward 19.0 Epsilon 0.02462665074849966\n",
      "Episode 7480 Reward 21.0 Epsilon 0.024503794175501366\n",
      "Episode 7490 Reward 27.0 Epsilon 0.02438155050507285\n",
      "Episode 7500 Reward 26.0 Epsilon 0.02425991667958724\n",
      "Episode 7510 Reward 22.0 Epsilon 0.024138889656671433\n",
      "Episode 7520 Reward 17.0 Epsilon 0.024018466409130017\n",
      "Episode 7530 Reward 27.0 Epsilon 0.02389864392486954\n",
      "Episode 7540 Reward 127.0 Epsilon 0.023779419206823176\n",
      "Episode 7550 Reward 32.0 Epsilon 0.023660789272875776\n",
      "Episode 7560 Reward 19.0 Epsilon 0.023542751155789254\n",
      "Episode 7570 Reward 110.0 Epsilon 0.023425301903128366\n",
      "Episode 7580 Reward 112.0 Epsilon 0.02330843857718689\n",
      "Episode 7590 Reward 113.0 Epsilon 0.023192158254914122\n",
      "Episode 7600 Reward 105.0 Epsilon 0.02307645802784178\n",
      "Episode 7610 Reward 101.0 Epsilon 0.022961335002011217\n",
      "Episode 7620 Reward 118.0 Epsilon 0.022846786297901107\n",
      "Episode 7630 Reward 202.0 Epsilon 0.02273280905035536\n",
      "Episode 7640 Reward 285.0 Epsilon 0.022619400408511466\n",
      "Episode 7650 Reward 282.0 Epsilon 0.022506557535729218\n",
      "Episode 7660 Reward 146.0 Epsilon 0.022394277609519724\n",
      "Episode 7670 Reward 116.0 Epsilon 0.022282557821474827\n",
      "Episode 7680 Reward 111.0 Epsilon 0.02217139537719687\n",
      "Episode 7690 Reward 115.0 Epsilon 0.022060787496228782\n",
      "Episode 7700 Reward 147.0 Epsilon 0.021950731411984545\n",
      "Episode 7710 Reward 152.0 Epsilon 0.02184122437167998\n",
      "Episode 7720 Reward 201.0 Epsilon 0.0217322636362639\n",
      "Episode 7730 Reward 436.0 Epsilon 0.02162384648034961\n",
      "Episode 7740 Reward 220.0 Epsilon 0.021515970192146724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7750 Reward 177.0 Epsilon 0.021408632073393337\n",
      "Episode 7760 Reward 167.0 Epsilon 0.021301829439288544\n",
      "Episode 7770 Reward 153.0 Epsilon 0.02119555961842528\n",
      "Episode 7780 Reward 170.0 Epsilon 0.0210898199527235\n",
      "Episode 7790 Reward 381.0 Epsilon 0.0209846077973637\n",
      "Episode 7800 Reward 500.0 Epsilon 0.020879920520720754\n",
      "Episode 7810 Reward 178.0 Epsilon 0.020775755504298096\n",
      "Episode 7820 Reward 148.0 Epsilon 0.02067211014266222\n",
      "Episode 7830 Reward 130.0 Epsilon 0.02056898184337753\n",
      "Episode 7840 Reward 168.0 Epsilon 0.020466368026941472\n",
      "Episode 7850 Reward 183.0 Epsilon 0.020364266126720022\n",
      "Episode 7860 Reward 169.0 Epsilon 0.020262673588883482\n",
      "Episode 7870 Reward 250.0 Epsilon 0.020161587872342634\n",
      "Episode 7880 Reward 407.0 Epsilon 0.020061006448685144\n",
      "Episode 7890 Reward 174.0 Epsilon 0.019960926802112325\n",
      "Episode 7900 Reward 191.0 Epsilon 0.019861346429376234\n",
      "Episode 7910 Reward 303.0 Epsilon 0.01976226283971702\n",
      "Episode 7920 Reward 401.0 Epsilon 0.01966367355480067\n",
      "Episode 7930 Reward 343.0 Epsilon 0.019565576108656998\n",
      "Episode 7940 Reward 476.0 Epsilon 0.01946796804761795\n",
      "Episode 7950 Reward 191.0 Epsilon 0.019370846930256244\n",
      "Episode 7960 Reward 251.0 Epsilon 0.019274210327324324\n",
      "Episode 7970 Reward 500.0 Epsilon 0.01917805582169356\n",
      "Episode 7980 Reward 307.0 Epsilon 0.019082381008293823\n",
      "Episode 7990 Reward 500.0 Epsilon 0.0189871834940533\n",
      "Episode 8000 Reward 330.0 Epsilon 0.018892460897838654\n",
      "Episode 8010 Reward 329.0 Epsilon 0.018798210850395472\n",
      "Episode 8020 Reward 500.0 Epsilon 0.01870443099428899\n",
      "Episode 8030 Reward 368.0 Epsilon 0.01861111898384512\n",
      "Episode 8040 Reward 500.0 Epsilon 0.018518272485091813\n",
      "Episode 8050 Reward 284.0 Epsilon 0.018425889175700642\n",
      "Episode 8060 Reward 468.0 Epsilon 0.01833396674492873\n",
      "Episode 8070 Reward 318.0 Epsilon 0.018242502893560962\n",
      "Episode 8080 Reward 344.0 Epsilon 0.01815149533385246\n",
      "Episode 8090 Reward 264.0 Epsilon 0.018060941789471374\n",
      "Episode 8100 Reward 313.0 Epsilon 0.01797083999544194\n",
      "Episode 8110 Reward 401.0 Epsilon 0.017881187698087816\n",
      "Episode 8120 Reward 294.0 Epsilon 0.017791982654975724\n",
      "Episode 8130 Reward 345.0 Epsilon 0.017703222634859364\n",
      "Episode 8140 Reward 500.0 Epsilon 0.017614905417623603\n",
      "Episode 8150 Reward 439.0 Epsilon 0.01752702879422892\n",
      "Episode 8160 Reward 500.0 Epsilon 0.01743959056665619\n",
      "Episode 8170 Reward 394.0 Epsilon 0.017352588547851693\n",
      "Episode 8180 Reward 500.0 Epsilon 0.017266020561672396\n",
      "Episode 8190 Reward 500.0 Epsilon 0.017179884442831535\n",
      "Episode 8200 Reward 491.0 Epsilon 0.017094178036844453\n",
      "Episode 8210 Reward 363.0 Epsilon 0.017008899199974704\n",
      "Episode 8220 Reward 340.0 Epsilon 0.016924045799180457\n",
      "Episode 8230 Reward 500.0 Epsilon 0.016839615712061114\n",
      "Episode 8240 Reward 500.0 Epsilon 0.01675560682680423\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m cum_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m#env.render()\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Choose next action according to the annealed e-greedy policy.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     state_, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action) \u001b[38;5;66;03m#Act on it.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m#state_ = np.reshape(state_, [1, input_shape]) \u001b[39;00m\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mAgent.choose_action\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m     21\u001b[0m state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39margmax(q_values)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\drl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mQnet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer(x))\n\u001b[0;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\drl\\lib\\site-packages\\torch\\nn\\functional.py:1442\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1440\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1442\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Now train the agent.\n",
    "\n",
    "#Hyperparameters.\n",
    "episodes = 10000\n",
    "target_fraction = 100\n",
    "\n",
    "#Some plotting.\n",
    "episode_number = []\n",
    "episode_rewards = []\n",
    "\n",
    "#Initialize the agent.\n",
    "agent = Agent()\n",
    "\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    #state = np.reshape(state, [1, input_shape])\n",
    "    \n",
    "    #Keep track of cumulative reward.\n",
    "    cum_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        #env.render()\n",
    "        action = agent.choose_action(state) #Choose next action according to the annealed e-greedy policy.\n",
    "        state_, reward, done, _ = env.step(action) #Act on it.\n",
    "        #state_ = np.reshape(state_, [1, input_shape]) \n",
    "        agent.memory.add((state, action, reward, state_, 1-done)) #Add the experience to the memory.\n",
    "        state = state_ #Progress the state to the next.\n",
    "        cum_reward += reward #Increase the reward.\n",
    "        \n",
    "        if done:\n",
    "            if i % target_fraction == 0:\n",
    "                agent.update_target_net() #Update the target net a fraction of the time.\n",
    "            #if agent.epsilon == e_min and cum_reward < np.mean(episode_rewards[-10:]):\n",
    "            #    pass\n",
    "            agent.learn() #Learn from the memory.\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(\"Episode {} Reward {} Epsilon {}\".format(i, cum_reward, agent.return_epsilon()))\n",
    "            break\n",
    "        \n",
    "            episode_number.append(i)\n",
    "            episode_rewards.append(cum_reward)\n",
    "\n",
    "plt.plot(episode_number, episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145083a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db1061a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Reward 500.0 Epsilon 0.001\n",
      "Episode 1 Reward 500.0 Epsilon 0.001\n",
      "Episode 2 Reward 500.0 Epsilon 0.001\n",
      "Episode 3 Reward 500.0 Epsilon 0.001\n",
      "Episode 4 Reward 477.0 Epsilon 0.001\n",
      "Episode 5 Reward 500.0 Epsilon 0.001\n",
      "Episode 6 Reward 500.0 Epsilon 0.001\n",
      "Episode 7 Reward 500.0 Epsilon 0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m cum_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(state) \u001b[38;5;66;03m#Choose next action according to the annealed e-greedy policy.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     state_, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action) \u001b[38;5;66;03m#Act on it.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\drl\\lib\\site-packages\\gym\\core.py:286\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\drl\\lib\\site-packages\\gym\\core.py:286\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\drl\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:260\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    259\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 260\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Initialize the agent.\n",
    "episodes = 1000\n",
    "target_fraction = 10\n",
    "\n",
    "#Some plotting.\n",
    "episode_number = []\n",
    "episode_rewards = []\n",
    "\n",
    "agent.epsilon = 0.001\n",
    "\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    #print(state)\n",
    "    #state = np.reshape(state, [1, input_shape])\n",
    "    \n",
    "    #Keep track of cumulative reward.\n",
    "    cum_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        env.render()\n",
    "        action = agent.choose_action(state) #Choose next action according to the annealed e-greedy policy.\n",
    "        state_, reward, done, _ = env.step(action) #Act on it.\n",
    "        #state_ = np.reshape(state_, [1, input_shape]) \n",
    "        state = state_ #Progress the state to the next.\n",
    "        cum_reward += reward #Increase the reward.\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode {} Reward {} Epsilon {}\".format(i, cum_reward, agent.return_epsilon()))    \n",
    "            episode_number.append(i)\n",
    "            episode_rewards.append(cum_reward)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "614edec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498.796"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e51ce7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   4.,   1.,   2.,   1.,   3.,   3.,   4.,   9., 972.]),\n",
       " array([399. , 409.1, 419.2, 429.3, 439.4, 449.5, 459.6, 469.7, 479.8,\n",
       "        489.9, 500. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAPsElEQVR4nO3cf6zd9V3H8edLOtgPFQrcNKxtLJHqXDRu5IZhMMtcdfJjWfljQ5K5VdZZo8zNYbJVY0IyNQE1g2GUpa7MomyAjIVmY5uk2zKXCeN2LGyAkxuEtbXQu/FDkUwlvP3jfHCHyy2l99ye23s/z0dycz6fz/dzzvfzyad93XM/53u+qSokSX34kcUegCRpfAx9SeqIoS9JHTH0Jakjhr4kdWTFYg/ghZx88sm1bt26xR6GJC0pu3fv/l5VTcx17KgO/XXr1jE1NbXYw5CkJSXJQwc7dsjtnSTXJDmQ5NtDbScmuS3J/e1xZWtPkquSTCe5O8npQ8/Z1Prfn2TTqJOSJB2+F7On/7fA2bPatgK7qmo9sKvVAc4B1refLcDVMPglAVwKvA44A7j02V8UkqTxOWToV9VXgEdnNW8EdrTyDuD8ofZra+B24IQkpwC/CtxWVY9W1WPAbTz/F4kk6Qib79U7q6pqfys/DKxq5dXAnqF+e1vbwdqfJ8mWJFNJpmZmZuY5PEnSXEa+ZLMGN+9ZsBv4VNW2qpqsqsmJiTk/fJYkzdN8Q/+Rtm1DezzQ2vcBa4f6rWltB2uXJI3RfEN/J/DsFTibgFuG2t/ZruI5E3iibQN9AXhTkpXtA9w3tTZJ0hgd8jr9JJ8E3gCcnGQvg6twLgNuTLIZeAi4oHW/FTgXmAaeAi4CqKpHk/wxcGfr96Gqmv3hsCTpCMvRfD/9ycnJ8stZknR4kuyuqsm5jh3V38iVpMW0butnF+3cD1523hF5XW+4JkkdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SerISKGf5P1J7kny7SSfTPLSJKcmuSPJdJIbkhzb+h7X6tPt+LoFmYEk6UWbd+gnWQ28F5isqp8FjgEuBC4Hrqiq04DHgM3tKZuBx1r7Fa2fJGmMRt3eWQG8LMkK4OXAfuCNwE3t+A7g/Fbe2Oq04xuSZMTzS5IOw7xDv6r2AX8BfJdB2D8B7AYer6qnW7e9wOpWXg3sac99uvU/afbrJtmSZCrJ1MzMzHyHJ0mawyjbOysZvHs/FXgl8Arg7FEHVFXbqmqyqiYnJiZGfTlJ0pBRtnd+Gfi3qpqpqv8FbgbOAk5o2z0Aa4B9rbwPWAvQjh8PfH+E80uSDtMoof9d4MwkL2978xuAe4EvAW9tfTYBt7TyzlanHf9iVdUI55ckHaZR9vTvYPCB7DeAb7XX2gZ8ELgkyTSDPfvt7SnbgZNa+yXA1hHGLUmahxWH7nJwVXUpcOms5geAM+bo+wPgbaOcT5I0Gr+RK0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0JakjI4V+khOS3JTkX5Lcl+QXkpyY5LYk97fHla1vklyVZDrJ3UlOX5gpSJJerFHf6X8E+HxVvQr4eeA+YCuwq6rWA7taHeAcYH372QJcPeK5JUmHad6hn+R44PXAdoCq+p+qehzYCOxo3XYA57fyRuDaGrgdOCHJKfM9vyTp8I3yTv9UYAb4eJK7knwsySuAVVW1v/V5GFjVyquBPUPP39vaniPJliRTSaZmZmZGGJ4kabZRQn8FcDpwdVW9FvgvfriVA0BVFVCH86JVta2qJqtqcmJiYoThSZJmGyX09wJ7q+qOVr+JwS+BR57dtmmPB9rxfcDaoeevaW2SpDGZd+hX1cPAniQ/3Zo2APcCO4FNrW0TcEsr7wTe2a7iORN4YmgbSJI0BitGfP7vAtclORZ4ALiIwS+SG5NsBh4CLmh9bwXOBaaBp1pfSdIYjRT6VfVNYHKOQxvm6FvAxaOcT5I0Gr+RK0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHRk59JMck+SuJJ9p9VOT3JFkOskNSY5t7ce1+nQ7vm7Uc0uSDs9CvNN/H3DfUP1y4IqqOg14DNjc2jcDj7X2K1o/SdIYjRT6SdYA5wEfa/UAbwRual12AOe38sZWpx3f0PpLksZk1Hf6VwIfAJ5p9ZOAx6vq6VbfC6xu5dXAHoB2/InWX5I0JvMO/SRvBg5U1e4FHA9JtiSZSjI1MzOzkC8tSd0b5Z3+WcBbkjwIXM9gW+cjwAlJVrQ+a4B9rbwPWAvQjh8PfH/2i1bVtqqarKrJiYmJEYYnSZpt3qFfVX9QVWuqah1wIfDFqno78CXgra3bJuCWVt7Z6rTjX6yqmu/5JUmH70hcp/9B4JIk0wz27Le39u3ASa39EmDrETi3JOkFrDh0l0Orqi8DX27lB4Az5ujzA+BtC3E+SdL8+I1cSeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR2Zd+gnWZvkS0nuTXJPkve19hOT3Jbk/va4srUnyVVJppPcneT0hZqEJOnFGeWd/tPA71fVq4EzgYuTvBrYCuyqqvXArlYHOAdY3362AFePcG5J0jzMO/Sran9VfaOV/xO4D1gNbAR2tG47gPNbeSNwbQ3cDpyQ5JT5nl+SdPgWZE8/yTrgtcAdwKqq2t8OPQysauXVwJ6hp+1tbbNfa0uSqSRTMzMzCzE8SVIzcugn+VHgU8DvVdV/DB+rqgLqcF6vqrZV1WRVTU5MTIw6PEnSkJFCP8lLGAT+dVV1c2t+5Nltm/Z4oLXvA9YOPX1Na5MkjckoV+8E2A7cV1UfHjq0E9jUypuAW4ba39mu4jkTeGJoG0iSNAYrRnjuWcA7gG8l+WZr+0PgMuDGJJuBh4AL2rFbgXOBaeAp4KIRzi1Jmod5h35VfRXIQQ5vmKN/ARfP93ySpNH5jVxJ6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHRl76Cc5O8l3kkwn2Tru80tSz1aM82RJjgH+CvgVYC9wZ5KdVXXvOMchaWlZt/Wziz2EZWOsoQ+cAUxX1QMASa4HNgLLKvQX6x/og5edtyjnBecsLRXjDv3VwJ6h+l7gdcMdkmwBtrTqk0m+s0DnPhn43gK91lEplz+nuuznC8+ZcxfzHdLbfKGzOefykeb7Ewc7MO7QP6Sq2gZsW+jXTTJVVZML/bpHK+e7vPU2X+hvzkdqvuP+IHcfsHaovqa1SZLGYNyhfyewPsmpSY4FLgR2jnkMktStsW7vVNXTSd4DfAE4Brimqu4Z0+kXfMvoKOd8l7fe5gv9zfmIzDdVdSReV5J0FPIbuZLUEUNfkjqyrEI/yTFJ7krymVY/Nckd7ZYPN7QPj0lyXKtPt+PrFnXg8zTHfK9rt7j4dpJrkryktSfJVW2+dyc5fXFHPj+z5zvUflWSJ4fqy3V9k+RPk/xrkvuSvHeofcmvL8w55w1JvpHkm0m+muS01r7k1zjJg0m+1eY21dpOTHJbkvvb48rWvmBrvKxCH3gfcN9Q/XLgiqo6DXgM2NzaNwOPtfYrWr+laPZ8rwNeBfwc8DLg3a39HGB9+9kCXD3GMS6k2fMlySSwcla/5bq+v8HgkudXVdXPANe39uWyvvD8OV8NvL2qXgN8Avij1r5c1viXquo1Q9fjbwV2VdV6YFerwwKu8bIJ/SRrgPOAj7V6gDcCN7UuO4DzW3ljq9OOb2j9l4zZ8wWoqlurAb7O4HsQMJjvte3Q7cAJSU4Z+6BHMNd8272c/hz4wKzuy3J9gd8GPlRVzwBU1YHWvuTXFw465wJ+vJWPB/69lZf8Gh/E8LxmZ9aCrPGyCX3gSgb/+Z9p9ZOAx6vq6Vbfy+A2EDB0O4h2/InWfym5kufO9/+1bZ13AJ9vTXPd/mL17Ocd5a7k+fN9D7CzqvbP6rtc1/cngV9LMpXkc0nWt/blsL4w95zfDdyaZC+Df9OXtfblsMYF/GOS3e32MwCrhv49PwysauUFW+NlEfpJ3gwcqKrdiz2WcXgR8/1r4CtV9U9jHNYRM9d8k7wSeBvwl4s2sCPkBdb3OOAHbSvgb4Brxj64I+QF5vx+4NyqWgN8HPjw2Ad35PxiVZ3OYOvm4iSvHz7Y/mJf8Gvqj7p778zTWcBbkpwLvJTBn4MfYfAn0Ir2TmD4lg/P3g5ib5IVDP5s/P74hz1vz5tvkr+vql9PcikwAfzWUP+lfvuLudb3HuC/gen2V/3Lk0y3Pd5lub4M3t3d3Pp8mkEIwtJfX5h7zp9l8PnFHa3PDfzwr9elvsZU1b72eCDJpxnchfiRJKdU1f62ffPsFt7CrXFVLasf4A3AZ1r5H4ALW/mjwO+08sXAR1v5QuDGxR73As333cDXgJfN6nMe8DkgwJnA1xd73Asx31ntTw6Vl+v6Xga8a6j9zuW2vsNzZvCm9HvAT7X2zcCnlsMaA68Afmyo/DXgbAafUW1t7VuBP1voNV4u7/QP5oPA9Un+BLgL2N7atwN/l2QaeJTBP5rl4KPAQ8A/t3e/N1fVh4BbgXOBaeAp4KJFG+F4LNf1vQy4Lsn7gSf54dVZy3J9a3Dblt8EPpXkGQZX4L2rHV7qa7wK+HT7f7oC+ERVfT7JncCNSTYz+L98Qeu/YGvsbRgkqSPL4oNcSdKLY+hLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0JekjvwfsbLyxzWYaQgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bc90353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('input_layer.weight',\n",
       "              tensor([[-0.1994,  0.1353,  0.1600,  0.3063],\n",
       "                      [ 0.3256, -0.1400,  0.4060,  0.1669],\n",
       "                      [-0.0010, -0.3522,  0.3870, -0.2000],\n",
       "                      [-0.2066,  0.3354,  0.4976, -0.4269],\n",
       "                      [-0.3998, -0.3609, -0.0830, -0.6328],\n",
       "                      [ 0.2482,  0.2487, -0.4317, -0.3149],\n",
       "                      [-0.0223,  0.1453, -0.1667, -0.1945],\n",
       "                      [-0.3056,  0.3136,  0.8232,  0.2452],\n",
       "                      [ 0.0820,  0.1227,  0.9625,  0.5969],\n",
       "                      [-0.2469,  0.2421,  0.0450,  0.3679],\n",
       "                      [-0.1733, -0.3154, -0.5466, -0.2301],\n",
       "                      [-0.2824, -0.1315,  0.5698,  0.3459],\n",
       "                      [ 0.3083,  0.5678,  0.6660,  0.2711],\n",
       "                      [ 0.1669,  0.0017,  0.4340,  0.0397],\n",
       "                      [-0.4403, -0.3519, -0.2125, -0.1339],\n",
       "                      [-0.0331, -0.5045, -0.1691,  0.0294],\n",
       "                      [ 0.1053,  0.0160, -0.1410, -0.1248],\n",
       "                      [-0.5149,  0.2123,  0.0711, -0.4740],\n",
       "                      [ 0.0059, -0.3041,  0.1550,  0.1240],\n",
       "                      [ 0.0531, -0.4017, -0.2598,  0.1907],\n",
       "                      [ 0.2678,  0.0379, -0.3248, -0.4556],\n",
       "                      [-0.1024, -0.6501, -0.0973,  0.0042],\n",
       "                      [-0.6109, -0.4244, -0.4984, -0.5840],\n",
       "                      [ 0.0239,  0.0437,  0.8135,  0.5440],\n",
       "                      [ 0.3846, -0.3133,  0.2408,  0.0018],\n",
       "                      [-0.0662, -0.1192,  0.4309, -0.3149],\n",
       "                      [ 0.4488, -0.1104, -0.6084,  0.0166],\n",
       "                      [ 0.0270, -0.2295, -0.2248,  0.0143],\n",
       "                      [ 0.0025, -0.0671,  0.2505,  0.2130],\n",
       "                      [-0.3606,  0.3005, -0.2969, -0.2376],\n",
       "                      [-0.3744, -0.4213,  0.3088, -0.0935],\n",
       "                      [ 0.2373, -0.4130, -0.3242,  0.2369],\n",
       "                      [ 0.2720,  0.0938,  0.1709, -0.0609],\n",
       "                      [-0.4835, -0.5955,  0.2893, -0.2379],\n",
       "                      [ 0.5745, -0.2237,  0.3035,  0.2086],\n",
       "                      [-0.0717,  0.1266, -0.0707, -0.0321],\n",
       "                      [-0.3457, -0.0207,  0.5700,  0.1575],\n",
       "                      [-0.2955,  0.2582,  0.0504,  0.3093],\n",
       "                      [-0.1790,  0.1729,  0.3871,  0.0579],\n",
       "                      [-0.4278, -0.5105, -0.3215, -0.4547],\n",
       "                      [-0.3768,  0.6304, -0.1309, -0.2772],\n",
       "                      [ 0.4251,  0.0654,  0.0160, -0.4692],\n",
       "                      [ 0.0145,  0.0875,  0.7682,  0.4044],\n",
       "                      [ 0.2095,  0.2071, -0.3408,  0.0958],\n",
       "                      [-0.1225,  0.3791,  0.2098,  0.3352],\n",
       "                      [ 0.2429,  0.6161,  0.6348,  0.1932],\n",
       "                      [-0.2474,  0.1274,  0.2608,  0.0320],\n",
       "                      [ 0.6104, -0.1117, -0.8783, -0.5431],\n",
       "                      [ 0.1760,  0.1522,  0.4758,  0.0444],\n",
       "                      [-0.5400,  0.2104, -0.1491,  0.4484],\n",
       "                      [-0.1231,  0.0987, -0.0747, -0.2505],\n",
       "                      [ 0.1014, -0.3541,  0.1454, -0.2606],\n",
       "                      [-0.0047, -0.1568, -0.3945,  0.0452],\n",
       "                      [ 0.4826,  0.0651,  0.0579,  0.4405],\n",
       "                      [ 0.4285, -0.1459, -0.4196, -0.0605],\n",
       "                      [ 0.3408, -0.2076, -0.2424,  0.0556],\n",
       "                      [-0.0746,  0.0296, -0.4411,  0.0597],\n",
       "                      [-0.4005,  0.4559,  0.0840,  0.3195],\n",
       "                      [ 0.2957, -0.4124,  0.1724, -0.2118],\n",
       "                      [ 0.6073, -0.1631,  0.3997, -0.2985],\n",
       "                      [ 0.0234, -0.2291, -0.2221, -0.0582],\n",
       "                      [ 0.1787,  0.1518,  0.1154,  0.0057],\n",
       "                      [ 0.1337,  0.5257,  0.2229, -0.2033],\n",
       "                      [-0.3845,  0.3734,  0.1529,  0.3409]])),\n",
       "             ('input_layer.bias',\n",
       "              tensor([ 0.5439, -0.2882,  0.2196,  0.0305, -0.2029, -0.4771,  0.0838, -0.0741,\n",
       "                      -0.1860,  0.1421, -0.4373, -0.3428, -0.1451,  0.4893, -0.1686,  0.2340,\n",
       "                       0.6763, -0.3686,  0.3389,  0.2844, -0.1393, -0.3630, -0.0087,  0.0683,\n",
       "                       0.6041,  0.5966,  0.2368,  0.2905,  0.5708,  0.7280,  0.5118,  0.4724,\n",
       "                       0.4468,  0.2145, -0.1309,  0.4197, -0.1461,  0.5512, -0.4991, -0.0678,\n",
       "                       0.3806,  0.2361, -0.1802,  0.3362,  0.6064, -0.0852,  0.5441, -0.3020,\n",
       "                      -0.3973,  0.0103,  0.1423,  0.3190,  0.3416, -0.2719,  0.2084,  0.3718,\n",
       "                       0.2089,  0.6619,  0.4954, -0.5439,  0.2383,  0.1868,  0.4511,  0.6799])),\n",
       "             ('hidden_layer.weight',\n",
       "              tensor([[-0.4068,  0.0839, -0.1366,  ...,  0.0954,  0.1224, -0.5602],\n",
       "                      [ 0.1358, -0.2980,  0.0814,  ...,  0.1738,  0.0319,  0.1364],\n",
       "                      [-0.0819, -0.0638, -0.0552,  ..., -0.0385, -0.0478, -0.0319],\n",
       "                      ...,\n",
       "                      [-0.2086,  0.2963,  0.0172,  ...,  0.0484,  0.0990, -0.2716],\n",
       "                      [ 0.0121, -0.0822, -0.0634,  ...,  0.0549, -0.0830, -0.0695],\n",
       "                      [-0.2908,  0.1482,  0.0697,  ...,  0.0132,  0.0802, -0.1744]])),\n",
       "             ('hidden_layer.bias',\n",
       "              tensor([-0.2209,  0.1400, -0.0752,  0.1561,  0.0982,  0.1921,  0.1918,  0.0711,\n",
       "                       0.1206,  0.1041,  0.0085, -0.2383, -0.1962,  0.0278,  0.0693,  0.1158,\n",
       "                      -0.1165, -0.0104,  0.1161,  0.1922,  0.0832, -0.0643,  0.1884,  0.1560,\n",
       "                       0.0974, -0.0148,  0.1333,  0.0673,  0.1199,  0.1113,  0.1706,  0.1408,\n",
       "                       0.0886,  0.0734,  0.1515,  0.0485,  0.1227,  0.0601,  0.1520,  0.0511,\n",
       "                      -0.1645,  0.0667,  0.0346,  0.2270, -0.0050,  0.2173, -0.0437,  0.1854,\n",
       "                       0.1088,  0.1889, -0.0451,  0.0701,  0.0658, -0.0479, -0.1084,  0.0974,\n",
       "                       0.0548,  0.1461, -0.1624,  0.2192,  0.0122, -0.1508,  0.0046,  0.0442])),\n",
       "             ('output_layer.weight',\n",
       "              tensor([[-0.4824,  0.1988, -0.1129,  0.2868,  0.1320,  0.1550,  0.2541,  0.3180,\n",
       "                        0.2099,  0.2572, -0.3367, -0.5106, -0.4312, -0.0480,  0.1329,  0.1717,\n",
       "                        0.0417,  0.1348,  0.3066,  0.2389,  0.3077,  0.0190,  0.2899,  0.1798,\n",
       "                       -0.3368, -0.2276, -0.1055,  0.2963,  0.2326,  0.2651,  0.1900,  0.3455,\n",
       "                        0.2051,  0.2825,  0.1047, -0.0706,  0.2445, -0.0709,  0.3095,  0.3114,\n",
       "                       -0.3539, -0.0329, -0.2687,  0.2306, -0.2872,  0.2974, -0.3373,  0.3011,\n",
       "                        0.2384,  0.1841, -0.2891,  0.2191, -0.2558, -0.1063, -0.3859,  0.2971,\n",
       "                        0.2342,  0.1153, -0.3123,  0.1536,  0.0737, -0.5604, -0.0681, -0.2367],\n",
       "                      [-0.4347,  0.2232, -0.1063,  0.1009,  0.2844,  0.2351,  0.2902,  0.3242,\n",
       "                        0.2794,  0.2699, -0.3426, -0.4031, -0.2813, -0.0658,  0.3040,  0.2981,\n",
       "                       -0.0259, -0.1571,  0.1470,  0.2538,  0.2364, -0.0954,  0.1484,  0.2812,\n",
       "                       -0.1957,  0.1071, -0.3616,  0.2152,  0.3012,  0.2215,  0.2480,  0.3186,\n",
       "                        0.2725,  0.2434,  0.1928, -0.3333,  0.2619,  0.0394,  0.3003,  0.1982,\n",
       "                       -0.2520, -0.0867, -0.0236,  0.1591, -0.3223,  0.1470, -0.0990,  0.1742,\n",
       "                        0.1314,  0.2438, -0.0749,  0.2939, -0.3905,  0.0727, -0.3823,  0.2368,\n",
       "                        0.1745,  0.2832, -0.1809,  0.2203, -0.0501, -0.5206,  0.1006, -0.3837]])),\n",
       "             ('output_layer.bias', tensor([0.1743, 0.0854]))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy_network.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c4c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
