{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5178d0aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/University/Semester 2/Reinforcement Learning/venv_RL/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "import gym\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57cc46e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "episodes = 10000\n",
    "hidden_size = 64\n",
    "observation_space = 4\n",
    "n_actions = 2\n",
    "\n",
    "learning_rate = 0.001\n",
    "gamma = 1\n",
    "sigma = 0.1\n",
    "update_frequency = 10\n",
    "depth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a764e837",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #Hyperparams to be pushed into a config.\n",
    "        self.sigma = sigma\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        input_shape = observation_space\n",
    "        output_shape = n_actions\n",
    "        \n",
    "        self.device = 'cpu' \n",
    "        self.input_shape = observation_space\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        self.input_layer = nn.Linear(self.input_shape, hidden_size)\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, self.output_shape)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.input_layer(x))\n",
    "        x = torch.nn.functional.relu(self.hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return torch.nn.functional.softmax(x, dim=1)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "    \n",
    "        probabilities = self.forward(state) + torch.normal(torch.tensor(0.0), torch.tensor(sigma), size=(1,2))\n",
    "        probabilities[probabilities < 0] = 0\n",
    "        \n",
    "        model = torch.distributions.Categorical(probabilities)\n",
    "        action = model.sample()\n",
    "        return action.item(), model.log_prob(action)\n",
    "    \n",
    "class CriticNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #Hyperparams to be pushed into a config.\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        input_shape = observation_space\n",
    "        output_shape = 1\n",
    "        \n",
    "        self.device = 'cpu' \n",
    "        self.input_shape = observation_space\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        self.input_layer = nn.Linear(self.input_shape, hidden_size)\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, self.output_shape)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.input_layer(x))\n",
    "        x = torch.nn.functional.relu(self.hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def critic_value(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        return self.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c2bee3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Trajectory:\n",
    "    def __init__(self, max_size=500):\n",
    "        self.rewards = deque(maxlen=max_size)\n",
    "        self.log_probabilities = deque(maxlen=max_size)\n",
    "        self.values = deque(maxlen=max_size)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        #These want to come from an argument parser.\n",
    "        self.bootstrap = True\n",
    "        self.baseline_subtract = True\n",
    "        \n",
    "        #And these hyperparams from a config.\n",
    "        self.gamma = gamma \n",
    "        self.depth = depth\n",
    "        self.update_frequency = update_frequency\n",
    "        \n",
    "        self.policy_network = PolicyNet()\n",
    "        self.critic_network = CriticNet()\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        \n",
    "        self.trajectory = Trajectory()\n",
    "        \n",
    "        self.actor_loss = deque(maxlen=self.update_frequency)\n",
    "        self.critic_loss = deque(maxlen=self.update_frequency)\n",
    "    \n",
    "    def calculate_discounted_rewards(self):\n",
    "        discounted_rewards = []\n",
    "        rewards = list(self.trajectory.rewards)\n",
    "        \n",
    "        if self.bootstrap:\n",
    "            for t in range(len(rewards)):\n",
    "                T = min(self.depth, len(rewards)-t)\n",
    "                discounts = [self.gamma**i for i in range(T)]\n",
    "                discounted_reward = np.sum([r*d for r,d in zip(rewards[t:t+T], discounts)])\n",
    "                if not t+T == len(rewards):\n",
    "                    discounted_reward += (self.gamma**T)*self.trajectory.values[t+T]\n",
    "                discounted_rewards.append(discounted_reward)\n",
    "        else:\n",
    "            for t in range(len(rewards)):\n",
    "                discounts = [self.gamma**i for i in range(len(rewards)-t)]\n",
    "                discounted_reward = np.sum([r*d for r,d in zip (rewards[t:], discounts)])\n",
    "                discounted_rewards.append(discounted_reward)\n",
    "        return discounted_rewards\n",
    "        \n",
    "    def calculate_loss(self):\n",
    "        discounted_rewards = torch.tensor(self.calculate_discounted_rewards(), dtype=torch.float32, \n",
    "                                          device=self.policy_network.device)\n",
    "        values = torch.cat(list(self.trajectory.values)).squeeze()\n",
    "        log_probs = torch.cat(list(self.trajectory.log_probabilities)).squeeze()\n",
    "        \n",
    "        if self.baseline_subtract:\n",
    "            advantages = discounted_rewards - values\n",
    "            actor_loss = -torch.sum(log_probs*advantages.detach())\n",
    "            \n",
    "        else:\n",
    "            actor_loss = -torch.sum(log_probs*discounted_rewards)\n",
    "        \n",
    "        critic_loss = self.loss_function(discounted_rewards, values)\n",
    "        \n",
    "        self.actor_loss.append(actor_loss)\n",
    "        self.critic_loss.append(critic_loss)\n",
    "        \n",
    "        self.trajectory.rewards.clear()\n",
    "        self.trajectory.log_probabilities.clear()\n",
    "        self.trajectory.values.clear()\n",
    "    \n",
    "    def learn(self):\n",
    "        actor_loss = torch.stack(list(self.actor_loss)).squeeze().mean()\n",
    "        self.policy_network.optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.policy_network.optimizer.step()\n",
    "        self.policy_network.sigma *= 0.996 #Try annealing the exploration. \n",
    "        \n",
    "        critic_loss = torch.stack(list(self.critic_loss)).squeeze().mean()\n",
    "        self.critic_network.optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_network.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e13b1514",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 Reward 21\n",
      "Episode 100 Reward 22\n",
      "Episode 150 Reward 29\n",
      "Episode 200 Reward 30\n",
      "Episode 250 Reward 37\n",
      "Episode 300 Reward 35\n",
      "Episode 350 Reward 42\n",
      "Episode 400 Reward 49\n",
      "Episode 450 Reward 59\n",
      "Episode 500 Reward 65\n",
      "Episode 550 Reward 77\n",
      "Episode 600 Reward 77\n",
      "Episode 650 Reward 99\n",
      "Episode 700 Reward 118\n",
      "Episode 750 Reward 145\n",
      "Episode 800 Reward 161\n",
      "Episode 850 Reward 219\n",
      "Episode 900 Reward 210\n",
      "Episode 950 Reward 242\n",
      "Episode 1000 Reward 223\n",
      "Episode 1050 Reward 264\n",
      "Episode 1100 Reward 273\n",
      "Episode 1150 Reward 326\n",
      "Episode 1200 Reward 383\n",
      "Episode 1250 Reward 438\n",
      "Episode 1300 Reward 426\n",
      "Episode 1350 Reward 312\n",
      "Episode 1400 Reward 426\n",
      "Episode 1450 Reward 467\n",
      "Episode 1500 Reward 412\n",
      "Episode 1550 Reward 443\n",
      "Episode 1600 Reward 466\n",
      "Episode 1650 Reward 468\n",
      "Episode 1700 Reward 448\n",
      "Episode 1750 Reward 469\n",
      "Episode 1800 Reward 462\n",
      "Episode 1850 Reward 463\n",
      "Episode 1900 Reward 476\n",
      "Episode 1950 Reward 477\n",
      "Episode 2000 Reward 379\n",
      "Episode 2050 Reward 350\n",
      "Episode 2100 Reward 352\n",
      "Episode 2150 Reward 413\n",
      "Episode 2200 Reward 451\n",
      "Episode 2250 Reward 473\n",
      "Episode 2300 Reward 486\n",
      "Episode 2350 Reward 490\n",
      "Episode 2400 Reward 399\n",
      "Episode 2450 Reward 433\n",
      "Episode 2500 Reward 486\n",
      "Episode 2550 Reward 405\n",
      "Episode 2600 Reward 415\n",
      "Episode 2650 Reward 468\n",
      "Episode 2700 Reward 495\n",
      "Episode 2750 Reward 472\n",
      "Episode 2800 Reward 473\n",
      "Episode 2850 Reward 474\n",
      "Episode 2900 Reward 483\n",
      "Episode 2950 Reward 396\n",
      "Episode 3000 Reward 155\n",
      "Episode 3050 Reward 130\n",
      "Episode 3100 Reward 212\n",
      "Episode 3150 Reward 437\n",
      "Episode 3200 Reward 494\n",
      "Episode 3250 Reward 496\n",
      "Episode 3300 Reward 473\n",
      "Episode 3350 Reward 491\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m cumulative_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# env.render()\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     action, log_probability \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     value \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mcritic_network\u001b[38;5;241m.\u001b[39mcritic_value(state)\n\u001b[1;32m     14\u001b[0m     state_next, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mPolicyNet.choose_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     29\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 31\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mnormal(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m), torch\u001b[38;5;241m.\u001b[39mtensor(sigma), size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     32\u001b[0m     probabilities[probabilities \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     34\u001b[0m     model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(probabilities)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mPolicyNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer(x))\n\u001b[0;32m---> 24\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/University/Semester 2/Reinforcement Learning/venv_RL/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/University/Semester 2/Reinforcement Learning/venv_RL/lib/python3.10/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = Agent()\n",
    "cumulative_rewards = []\n",
    "print_freq = 50\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    cumulative_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        # env.render()\n",
    "        action, log_probability = agent.policy_network.choose_action(state)\n",
    "        value = agent.critic_network.critic_value(state)\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        agent.trajectory.rewards.append(reward)\n",
    "        agent.trajectory.log_probabilities.append(log_probability)\n",
    "        agent.trajectory.values.append(value)\n",
    "        state = state_next\n",
    "        cumulative_reward += reward\n",
    "\n",
    "        if done:\n",
    "            agent.calculate_loss()\n",
    "            cumulative_rewards.append(cumulative_reward)\n",
    "            if episode % agent.update_frequency == 0 and not episode == 0:\n",
    "                agent.learn()\n",
    "                if episode % print_freq == 0:\n",
    "                    print(f'Episode {episode} Reward {round(np.mean(cumulative_rewards[-print_freq:]))}') #.format(int(episode/agent.update_frequency), cumulative_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf19c39b-059c-4d03-8909-c5d5245c74b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
